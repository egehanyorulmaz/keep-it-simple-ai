{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27e56a18-cfb0-40e3-b12c-c72ae10bba83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from datasets import load_dataset\n",
    "from functools import partial\n",
    "import os\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed, Trainer, TrainingArguments, BitsAndBytesConfig, \\\n",
    "    DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3fd3ffe-834d-462f-8b56-829f9adf5f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name):\n",
    "    n_gpus = torch.cuda.device_count()\n",
    "    max_memory = f'{15960}MB'\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=\"auto\", # dispatch efficiently the model on the available ressources\n",
    "        max_memory = {i: max_memory for i in range(n_gpus)},\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=True)\n",
    "\n",
    "    # Needed for LLaMA tokenizer\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "789e8f9f-5ec5-4fab-b8cc-deb2c9cb7c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset json (/home/jupyter/.cache/huggingface/datasets/databricks___json/databricks--databricks-dolly-15k-7427aa6e57c34282/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n"
     ]
    }
   ],
   "source": [
    "# Load the databricks dataset from Hugging Face\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71ef5aad-f204-48da-9238-922dba86b6a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of prompts: 15011\n",
      "Column names are: ['instruction', 'context', 'response', 'category']\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of prompts: {len(dataset)}')\n",
    "print(f'Column names are: {dataset.column_names}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "362911ac-b1a2-43f9-ac46-5af7294f7a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When did Virgin Australia start operating?\n",
      "\n",
      "Virgin Australia, the trading name of Virgin Australia Airlines Pty Ltd, is an Australian-based airline. It is the largest airline by fleet size to use the Virgin brand. It commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route. It suddenly found itself as a major airline in Australia's domestic market after the collapse of Ansett Australia in September 2001. The airline has since grown to directly serve 32 cities in Australia, from hubs in Brisbane, Melbourne and Sydney.\n"
     ]
    }
   ],
   "source": [
    "def get_sample(idx):\n",
    "    instruction = dataset[\"instruction\"][idx]\n",
    "    context = dataset[\"context\"][idx]\n",
    "\n",
    "    print(instruction)\n",
    "    print()\n",
    "    print(context)\n",
    "    return instruction, context\n",
    "\n",
    "instruction, context = get_sample(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "494e483e-0ed6-4e2f-9141-cf328de1f82e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When did Virgin Australia start operating?\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"instruction\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4f893ed8-1506-45da-9fde-c61320a97110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Virgin Australia, the trading name of Virgin Australia Airlines Pty Ltd, is an Australian-based airline. It is the largest airline by fleet size to use the Virgin brand. It commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route. It suddenly found itself as a major airline in Australia's domestic market after the collapse of Ansett Australia in September 2001. The airline has since grown to directly serve 32 cities in Australia, from hubs in Brisbane, Melbourne and Sydney.\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"context\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79d73176-2a16-43fc-99e8-f55fffe3a7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"meta-llama/Llama-2-7b-hf\" \n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f9eebae-d710-4cd9-b7a7-eb36912cdb16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "453e1ad1683349e1915399a91203a1bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:655: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = load_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2d2515b-5e8a-4219-8c06-f3d6be40ea62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_inference(instruction, context = None):\n",
    "    if context:\n",
    "        prompt = f\"Below is an instruction that describes a task, paired with an input that provides further context.\\n\\n### Instruction: \\n{instruction}\\n\\n### Input: \\n{context}\\n\\n### Response: \\n\"\n",
    "    else:\n",
    "        prompt = f\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction: \\n{instruction}\\n\\n### Response: \\n\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", return_token_type_ids=False).to(\"cuda:0\")\n",
    "    # outputs = base_model.generate(**inputs, max_new_tokens=100)\n",
    "    # display(Markdown((tokenizer.decode(outputs[0], skip_special_tokens=True))))\n",
    "    outputs = model.generate(**inputs, max_new_tokens=300)\n",
    "    # print(\"---- NON-INSTRUCT-TUNED-MODEL ----\")\n",
    "    display(Markdown((tokenizer.decode(outputs[0], skip_special_tokens=True))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a5851c59-a8d4-4ed5-a7ed-4404dbca6b37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Below is an instruction that describes a task, paired with an input that provides further context.\n",
       "\n",
       "### Instruction: \n",
       "When did Virgin Australia start operating?\n",
       "\n",
       "### Input: \n",
       "Virgin Australia, the trading name of Virgin Australia Airlines Pty Ltd, is an Australian-based airline. It is the largest airline by fleet size to use the Virgin brand. It commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route. It suddenly found itself as a major airline in Australia's domestic market after the collapse of Ansett Australia in September 2001. The airline has since grown to directly serve 32 cities in Australia, from hubs in Brisbane, Melbourne and Sydney.\n",
       "\n",
       "### Response: \n",
       "Virgin Australia commenced operating on 31 August 2000 as Virgin Blue.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "instruction, context = get_sample(0)\n",
    "make_inference(instruction=instruction, context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "96e9edf4-df44-4106-8cb3-b3df6d386c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which is a species of fish? Tope or Rope\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
       "\n",
       "### Instruction: \n",
       "Which is a species of fish? Tope or Rope\n",
       "\n",
       "### Response: \n",
       "Tope is a species of fish\n",
       "\n",
       "### Instruction: \n",
       "Which is a species of fish? Tope or Rope\n",
       "\n",
       "### Response: \n",
       "Tope is a species of fish\n",
       "\n",
       "### Instruction: \n",
       "Which is a species of fish? Tope or Rope\n",
       "\n",
       "### Response: \n",
       "Tope is a species of fish\n",
       "\n",
       "### Instruction: \n",
       "Which is a species of fish? Tope or Rope\n",
       "\n",
       "### Response: \n",
       "Tope is a species of fish\n",
       "\n",
       "### Instruction: \n",
       "Which is a species of fish? Tope or Rope\n",
       "\n",
       "### Response: \n",
       "Tope is a species of fish\n",
       "\n",
       "### Instruction: \n",
       "Which is a species of fish? Tope or Rope\n",
       "\n",
       "### Response: \n",
       "Tope is a species of fish\n",
       "\n",
       "### Instruction: \n",
       "Which is a species of fish? Tope or Rope\n",
       "\n",
       "### Response: \n",
       "Tope is a species of fish\n",
       "\n",
       "### Instruction: \n",
       "Which is a species of fish? Tope or Rope\n",
       "\n",
       "### Response: \n",
       "Tope is a species of fish\n",
       "\n",
       "### Instruction: \n",
       "Which is a species of fish? Tope or Rope\n",
       "\n",
       "### Response: \n",
       "Tope is a"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "instruction, context = get_sample(1)\n",
    "make_inference(instruction=instruction, context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "88404ac8-142f-4e8c-9114-1b58930130ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why can camels survive for long without water?\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
       "\n",
       "### Instruction: \n",
       "Why can camels survive for long without water?\n",
       "\n",
       "### Response: \n",
       "Camels can survive long without water because they can store large amounts of water in their humps.\n",
       "\n",
       "### Instruction: \n",
       "What is the difference between a camel and a horse?\n",
       "\n",
       "### Response: \n",
       "Camels have a hump on their backs and horses do not.\n",
       "\n",
       "### Instruction: \n",
       "How do camels store water?\n",
       "\n",
       "### Response: \n",
       "Camels store water in their humps.\n",
       "\n",
       "### Instruction: \n",
       "Why do camels have humps?\n",
       "\n",
       "### Response: \n",
       "Camels have humps to store water.\n",
       "\n",
       "### Instruction: \n",
       "Why do camels have long eyelashes?\n",
       "\n",
       "### Response: \n",
       "Camels have long eyelashes to protect their eyes from sand.\n",
       "\n",
       "### Instruction: \n",
       "Why do camels have long necks?\n",
       "\n",
       "### Response: \n",
       "Camels have long necks to reach food in tall grass.\n",
       "\n",
       "### Instruction: \n",
       "How do camels sleep?\n",
       "\n",
       "### Response: \n",
       "Camels sleep standing up.\n",
       "\n",
       "### Instruction: \n",
       "How do camels protect themselves from the sun?\n",
       "\n",
       "### Response: \n",
       "Camels protect themselves from the sun by staying in the shade during the day.\n",
       "\n",
       "### Instruction: \n",
       "What"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "instruction, context = get_sample(2)\n",
    "make_inference(instruction=instruction, context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d34d19fc-af42-4637-bcc8-f9e2867ec442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice's parents have three daughters: Amy, Jessy, and what’s the name of the third daughter?\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
       "\n",
       "### Instruction: \n",
       "Alice's parents have three daughters: Amy, Jessy, and what’s the name of the third daughter?\n",
       "\n",
       "### Response: \n",
       "Jennifer\n",
       "\n",
       "### Instruction: \n",
       "Alice's parents have three daughters: Amy, Jessy, and what’s the name of the third daughter?\n",
       "\n",
       "### Response: \n",
       "Jennifer\n",
       "\n",
       "### Instruction: \n",
       "Alice's parents have three daughters: Amy, Jessy, and what’s the name of the third daughter?\n",
       "\n",
       "### Response: \n",
       "Jennifer\n",
       "\n",
       "### Instruction: \n",
       "Alice's parents have three daughters: Amy, Jessy, and what’s the name of the third daughter?\n",
       "\n",
       "### Response: \n",
       "Jennifer\n",
       "\n",
       "### Instruction: \n",
       "Alice's parents have three daughters: Amy, Jessy, and what’s the name of the third daughter?\n",
       "\n",
       "### Response: \n",
       "Jennifer\n",
       "\n",
       "### Instruction: \n",
       "Alice's parents have three daughters: Amy, Jessy, and what’s the name of the third daughter?\n",
       "\n",
       "### Response: \n",
       "Jennifer\n",
       "\n",
       "### Instruction: \n",
       "Alice's parents have three daughters: Amy, Jessy, and what’s the name of the third daughter?\n",
       "\n",
       "### Response: \n",
       "Jennifer\n",
       "\n",
       "### Instruction: \n",
       "Alice's parents have three daughters: Amy, Jessy, and what’s"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "instruction, context = get_sample(3)\n",
    "make_inference(instruction=instruction, context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d12912-915e-4e62-a016-65231d353aed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce891358-46c5-42d5-bcaf-6742d272fa23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.2-0.m111",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.2-0:m111"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
