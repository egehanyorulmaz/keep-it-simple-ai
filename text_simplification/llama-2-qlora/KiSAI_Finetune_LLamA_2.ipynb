{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa4483c6-f5ee-4812-aaaa-0b282c140174",
   "metadata": {},
   "source": [
    "### Fine-tuning LLama 2 with QLoRA\n",
    "https://blog.ovhcloud.com/fine-tuning-llama-2-models-using-a-single-gpu-qlora-and-ai-notebooks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9622d6-6105-499a-a3e5-921a1288b5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements_ovhcloud.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fa3de9-9a50-44f8-8970-bc5941b3c58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!lsb_release -a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71832104-9f40-4c40-90d6-8858d981d397",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export CUDA_VISIBLE_DEVICES=0,1,2,3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f2a5f9-ca7b-4871-bd5a-e29abbf5dc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo chmod 777 /root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92f29f14-c65c-4f96-9e6d-9a291cf1d543",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import bitsandbytes as bnb\n",
    "from datasets import load_dataset, Dataset\n",
    "from functools import partial\n",
    "import os\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, AutoPeftModelForCausalLM\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed, Trainer, TrainingArguments, BitsAndBytesConfig, \\\n",
    "    DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d810cd34-e96a-4ce7-883d-83cc92d34b96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'11GB'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'{int(torch.cuda.mem_get_info()[0]/1024**3)-2}GB'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b7f95fe-ec6e-4d5a-97df-3f57c93fcb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name, bnb_config):\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\", # dispatch efficiently the model on the available ressources  \n",
    "        load_in_8bit=True,\n",
    "        #max_memory=f'{int(torch.cuda.mem_get_info()[0]/1024**3)-2}GB',\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=True)\n",
    "\n",
    "    # Needed for LLaMA tokenizer\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1231971-178d-40e1-a91c-e8520de2408c",
   "metadata": {},
   "source": [
    "### Download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "891f6f1b-1c64-4a1b-a133-31f397e461c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>source_level_og</th>\n",
       "      <th>target_level_og</th>\n",
       "      <th>data_source</th>\n",
       "      <th>id</th>\n",
       "      <th>source_level_cefr</th>\n",
       "      <th>target_level_cefr</th>\n",
       "      <th>model</th>\n",
       "      <th>prompt</th>\n",
       "      <th>generated_text</th>\n",
       "      <th>cefr_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Donald Trump is interested in buying Greenland...</td>\n",
       "      <td>Donald Trump is interested in buying Greenland...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>BreakingNewsEnglish</td>\n",
       "      <td>TS000001588</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>flant5</td>\n",
       "      <td>Simplify</td>\n",
       "      <td>donald trump has said he would be interested i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Everyone knows that children don't like eating...</td>\n",
       "      <td>Everyone knows children don't like eating gree...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>BreakingNewsEnglish</td>\n",
       "      <td>TS000001750</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>flant5</td>\n",
       "      <td>Simplify</td>\n",
       "      <td>children dislike greens</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Archaeologists have found a big cemetery under...</td>\n",
       "      <td>Archaeologists found a big cemetery under the ...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>BreakingNewsEnglish</td>\n",
       "      <td>TS000001715</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>flant5</td>\n",
       "      <td>Simplify</td>\n",
       "      <td>egypt s government said we will need at least ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Baby orangutans in Indonesia are going to scho...</td>\n",
       "      <td>Baby orangutans in Indonesia are going to scho...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>BreakingNewsEnglish</td>\n",
       "      <td>TS000001895</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>flant5</td>\n",
       "      <td>Simplify</td>\n",
       "      <td>international animal rescue iar is a charity t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>New research says bald men should not worry ab...</td>\n",
       "      <td>Research says bald men should not worry or get...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>BreakingNewsEnglish</td>\n",
       "      <td>TS000001756</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>flant5</td>\n",
       "      <td>Simplify</td>\n",
       "      <td>men get stressed about losing their hair</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              source  \\\n",
       "0  Donald Trump is interested in buying Greenland...   \n",
       "1  Everyone knows that children don't like eating...   \n",
       "2  Archaeologists have found a big cemetery under...   \n",
       "3  Baby orangutans in Indonesia are going to scho...   \n",
       "4  New research says bald men should not worry ab...   \n",
       "\n",
       "                                              target  source_level_og  \\\n",
       "0  Donald Trump is interested in buying Greenland...              2.0   \n",
       "1  Everyone knows children don't like eating gree...              2.0   \n",
       "2  Archaeologists found a big cemetery under the ...              2.0   \n",
       "3  Baby orangutans in Indonesia are going to scho...              2.0   \n",
       "4  Research says bald men should not worry or get...              2.0   \n",
       "\n",
       "   target_level_og          data_source           id  source_level_cefr  \\\n",
       "0              1.0  BreakingNewsEnglish  TS000001588                  1   \n",
       "1              1.0  BreakingNewsEnglish  TS000001750                  1   \n",
       "2              1.0  BreakingNewsEnglish  TS000001715                  1   \n",
       "3              1.0  BreakingNewsEnglish  TS000001895                  2   \n",
       "4              1.0  BreakingNewsEnglish  TS000001756                  1   \n",
       "\n",
       "   target_level_cefr   model    prompt  \\\n",
       "0                  1  flant5  Simplify   \n",
       "1                  1  flant5  Simplify   \n",
       "2                  1  flant5  Simplify   \n",
       "3                  1  flant5  Simplify   \n",
       "4                  1  flant5  Simplify   \n",
       "\n",
       "                                      generated_text  cefr_labels  \n",
       "0  donald trump has said he would be interested i...            1  \n",
       "1                            children dislike greens            0  \n",
       "2  egypt s government said we will need at least ...            1  \n",
       "3  international animal rescue iar is a charity t...            1  \n",
       "4           men get stressed about losing their hair            0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "PATH = \"gs://kisai-data-msca310019-capstone/Text_Simplification/simplified_df_cefr_labeled.csv\"\n",
    "data = pd.read_csv(PATH)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49df3e72-5179-4098-8f61-79d5a05507f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length before dropping nan 3000\n",
      "Length after dropping nan 2400\n"
     ]
    }
   ],
   "source": [
    "print(\"Length before dropping nan\", len(data))\n",
    "data.dropna(subset=['target_level_og', 'source_level_og'], inplace=True)\n",
    "print(\"Length after dropping nan\", len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49cd5619-bdb9-4b5d-844f-bd18c116f19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_instruction_column(source_cefr, target_cefr):\n",
    "    return f\"Simplify the following context from CEFR Level {int(source_cefr)} to CEFR Level {int(target_cefr)}\"\n",
    "\n",
    "data[\"instruction\"] = data.apply(lambda row: generate_instruction_column(row[\"source_level_og\"], row[\"target_level_og\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9fa3260-011d-4a1c-9c7a-6580d40171f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Simplify the following context from CEFR Level 2 to CEFR Level 1'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[0][\"instruction\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20b90c44-7153-4b50-963b-248e7c149047",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.rename({'source': 'context', 'target': 'response'}, axis=1, inplace=True)\n",
    "data = data[[\"instruction\", \"context\", \"response\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "626c0885-e974-49d1-a39d-4b86773bd39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the DataFrame to a dictionary\n",
    "data_dict = data.to_dict('list')\n",
    "\n",
    "# Convert the dictionary to a Hugging Face Dataset\n",
    "dataset = Dataset.from_dict(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e86b2bf9-5609-45da-86aa-85dbfac8f7ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'context', 'response'],\n",
       "    num_rows: 2400\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9ddc696-c466-4d01-af19-a325ddda598f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of prompts: 2400\n",
      "Column names are: ['instruction', 'context', 'response']\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of prompts: {len(dataset)}')\n",
    "print(f'Column names are: {dataset.column_names}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad53a0ba-18a3-4468-aa48-fc78529ff6dd",
   "metadata": {},
   "source": [
    "### Preprocess the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a13a1d7-7514-4bae-bcaa-139d2bda1fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt_formats(sample):\n",
    "    \"\"\"\n",
    "    Format various fields of the sample ('instruction', 'context', 'response')\n",
    "    Then concatenate them using two newline characters \n",
    "    :param sample: Sample dictionnary\n",
    "    \"\"\"\n",
    "\n",
    "    INTRO_BLURB = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
    "    INSTRUCTION_KEY = \"### Instruction:\"\n",
    "    INPUT_KEY = \"Input:\"\n",
    "    RESPONSE_KEY = \"### Response:\"\n",
    "    END_KEY = \"### End\"\n",
    "    \n",
    "    blurb = f\"{INTRO_BLURB}\"\n",
    "    instruction = f\"{INSTRUCTION_KEY}\\n{sample['instruction']}\"\n",
    "    input_context = f\"{INPUT_KEY}\\n{sample['context']}\" if sample[\"context\"] else None\n",
    "    response = f\"{RESPONSE_KEY}\\n{sample['response']}\"\n",
    "    end = f\"{END_KEY}\"\n",
    "    \n",
    "    parts = [part for part in [blurb, instruction, input_context, response, end] if part]\n",
    "\n",
    "    formatted_prompt = \"\\n\\n\".join(parts)\n",
    "    \n",
    "    sample[\"text\"] = formatted_prompt\n",
    "\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e082d55-bc40-486d-a88b-8fa7a3feab62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOURCE https://github.com/databrickslabs/dolly/blob/master/training/trainer.py\n",
    "def get_max_length(model):\n",
    "    conf = model.config\n",
    "    max_length = None\n",
    "    for length_setting in [\"n_positions\", \"max_position_embeddings\", \"seq_length\"]:\n",
    "        max_length = getattr(model.config, length_setting, None)\n",
    "        if max_length:\n",
    "            print(f\"Found max lenth: {max_length}\")\n",
    "            break\n",
    "    if not max_length:\n",
    "        max_length = 1024\n",
    "        print(f\"Using default max length: {max_length}\")\n",
    "    return max_length\n",
    "\n",
    "\n",
    "def preprocess_batch(batch, tokenizer, max_length):\n",
    "    \"\"\"\n",
    "    Tokenizing a batch\n",
    "    \"\"\"\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "\n",
    "# SOURCE https://github.com/databrickslabs/dolly/blob/master/training/trainer.py\n",
    "def preprocess_dataset(tokenizer: AutoTokenizer, max_length: int, seed, dataset: str):\n",
    "    \"\"\"Format & tokenize it so it is ready for training\n",
    "    :param tokenizer (AutoTokenizer): Model Tokenizer\n",
    "    :param max_length (int): Maximum number of tokens to emit from tokenizer\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add prompt to each sample\n",
    "    print(\"Preprocessing dataset...\")\n",
    "    dataset = dataset.map(create_prompt_formats)#, batched=True)\n",
    "    \n",
    "    # Apply preprocessing to each batch of the dataset & and remove 'instruction', 'context', 'response', 'category' fields\n",
    "    _preprocessing_function = partial(preprocess_batch, max_length=max_length, \n",
    "                                      tokenizer=tokenizer)\n",
    "    dataset = dataset.map(\n",
    "        _preprocessing_function,\n",
    "        batched=True,\n",
    "        remove_columns=[\"instruction\", \"context\", \"response\", \"text\"],\n",
    "    )\n",
    "\n",
    "    # Filter out samples that have input_ids exceeding max_length\n",
    "    dataset = dataset.filter(lambda sample: len(sample[\"input_ids\"]) < max_length)\n",
    "    \n",
    "    # Shuffle dataset\n",
    "    dataset = dataset.shuffle(seed=seed)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb15376-0114-46b7-9840-3bc16ce5e2f8",
   "metadata": {},
   "source": [
    "### Create a bitsandbytes configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e27bd92b-65ba-4b59-8428-cf83ca310a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bnb_config():\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    )\n",
    "\n",
    "    return bnb_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "508ab050-003f-48a2-b478-e2e3c1729252",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_peft_config(modules):\n",
    "    \"\"\"\n",
    "    Create Parameter-Efficient Fine-Tuning config for your model\n",
    "    :param modules: Names of the modules to apply Lora to\n",
    "    \"\"\"\n",
    "    config = LoraConfig(\n",
    "        r=16,  # dimension of the updated matrices\n",
    "        lora_alpha=64,  # parameter for scaling\n",
    "        target_modules=modules,\n",
    "        lora_dropout=0.1,  # dropout probability for layers\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "108b7ee8-9de1-4e47-bfab-845713429540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOURCE https://github.com/artidoro/qlora/blob/main/qlora.py\n",
    "\n",
    "def find_all_linear_names(model):\n",
    "    cls = bnb.nn.Linear4bit #if args.bits == 4 else (bnb.nn.Linear8bitLt if args.bits == 8 else torch.nn.Linear)\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "\n",
    "    if 'lm_head' in lora_module_names:  # needed for 16-bit\n",
    "        lora_module_names.remove('lm_head')\n",
    "    return list(lora_module_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "abe096bc-64db-49d2-a897-f0ba8556d159",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model, use_4bit=False):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        num_params = param.numel()\n",
    "        # if using DS Zero 3 and the weights are initialized empty\n",
    "        if num_params == 0 and hasattr(param, \"ds_numel\"):\n",
    "            num_params = param.ds_numel\n",
    "\n",
    "        all_param += num_params\n",
    "        if param.requires_grad:\n",
    "            trainable_params += num_params\n",
    "    if use_4bit:\n",
    "        trainable_params /= 2\n",
    "    print(\n",
    "        f\"all params: {all_param:,d} || trainable params: {trainable_params:,d} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b07cd5-37b1-42b0-a20f-edf8bce43b94",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6e4f2301-a0f2-4b6c-be72-20031add62c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"meta-llama/Llama-2-7b-hf\" \n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6cff6c67-3cd6-4082-8494-f2331f8751bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9d145d3856d456c871c8c7f18479a9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:655: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load model from HF with user's token and with bitsandbytes config\n",
    "\n",
    "bnb_config = create_bnb_config()\n",
    "\n",
    "model, tokenizer = load_model(model_name, bnb_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b3524ee7-e11a-4442-9276-77b8cb8c25c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found max lenth: 4096\n",
      "Preprocessing dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/2400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Preprocess dataset\n",
    "\n",
    "max_length = get_max_length(model)\n",
    "\n",
    "dataset = preprocess_dataset(tokenizer, max_length, seed, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d519a5dd-091c-4db6-8ddb-eb5baaf5f9b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask'],\n",
       "    num_rows: 2400\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d70081d4-ab3c-4ec2-98bd-71335910b2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, tokenizer, dataset, output_dir):\n",
    "    # Apply preprocessing to the model to prepare it by\n",
    "    # 1 - Enabling gradient checkpointing to reduce memory usage during fine-tuning\n",
    "    model.gradient_checkpointing_enable()\n",
    "\n",
    "    # 2 - Using the prepare_model_for_kbit_training method from PEFT\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "    # Get lora module names\n",
    "    modules = find_all_linear_names(model)\n",
    "\n",
    "    # Create PEFT config for these modules and wrap the model to PEFT\n",
    "    peft_config = create_peft_config(modules)\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    \n",
    "    # Print information about the percentage of trainable parameters\n",
    "    print_trainable_parameters(model)\n",
    "    \n",
    "    # Training parameters\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        train_dataset=dataset,\n",
    "        args=TrainingArguments(\n",
    "            per_device_train_batch_size=1,\n",
    "            gradient_accumulation_steps=4,\n",
    "            warmup_steps=2,\n",
    "            max_steps=20,\n",
    "            learning_rate=2e-4,\n",
    "            fp16=True,\n",
    "            logging_steps=1,\n",
    "            output_dir=\"outputs\",\n",
    "            optim=\"paged_adamw_8bit\",\n",
    "        ),\n",
    "        data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "    )\n",
    "    \n",
    "    model.config.use_cache = False  # re-enable for inference to speed up predictions for similar inputs\n",
    "    \n",
    "    ### SOURCE https://github.com/artidoro/qlora/blob/main/qlora.py\n",
    "    # Verifying the datatypes before training\n",
    "    \n",
    "    dtypes = {}\n",
    "    for _, p in model.named_parameters():\n",
    "        dtype = p.dtype\n",
    "        if dtype not in dtypes: dtypes[dtype] = 0\n",
    "        dtypes[dtype] += p.numel()\n",
    "    total = 0\n",
    "    for k, v in dtypes.items(): total+= v\n",
    "    for k, v in dtypes.items():\n",
    "        print(k, v, v/total)\n",
    "     \n",
    "    do_train = True\n",
    "    \n",
    "    # Launch training\n",
    "    print(\"Training...\")\n",
    "    \n",
    "    if do_train:\n",
    "        train_result = trainer.train()\n",
    "        metrics = train_result.metrics\n",
    "        trainer.log_metrics(\"train\", metrics)\n",
    "        trainer.save_metrics(\"train\", metrics)\n",
    "        trainer.save_state()\n",
    "        print(metrics)    \n",
    "    \n",
    "    ###\n",
    "    \n",
    "    # Saving model\n",
    "    print(\"Saving last checkpoint of the model...\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    trainer.model.save_pretrained(output_dir)\n",
    "    \n",
    "    # Free memory for merging weights\n",
    "    del model\n",
    "    del trainer\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fc69099a-80d7-429f-92ab-959889d5ecdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:XRT configuration not detected. Defaulting to preview PJRT runtime. To silence this warning and continue using PJRT, explicitly set PJRT_DEVICE to a supported device or configure XRT. To disable default device selection, set PJRT_SELECT_DEFAULT_DEVICE=0\n",
      "WARNING:root:For more information about the status of PJRT, see https://github.com/pytorch/xla/blob/master/docs/pjrt.md\n",
      "WARNING:root:Defaulting to PJRT_DEVICE=CPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all params: 3,540,389,888 || trainable params: 39,976,960 || trainable%: 1.1291682911958425\n",
      "torch.float32 302387200 0.08541070604255438\n",
      "torch.uint8 3238002688 0.9145892939574456\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33megehanyorulmaz\u001b[0m (\u001b[33mkeep-it-simple-ai\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jupyter/keep-it-simple-ai/text_simplification/llama-2-qlora/wandb/run-20231014_203210-1c8fkmao</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/keep-it-simple-ai/huggingface/runs/1c8fkmao' target=\"_blank\">crimson-wave-4</a></strong> to <a href='https://wandb.ai/keep-it-simple-ai/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/keep-it-simple-ai/huggingface' target=\"_blank\">https://wandb.ai/keep-it-simple-ai/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/keep-it-simple-ai/huggingface/runs/1c8fkmao' target=\"_blank\">https://wandb.ai/keep-it-simple-ai/huggingface/runs/1c8fkmao</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20/20 03:00, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.897300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.866300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.699200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.539900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.317800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.134400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.090700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.077100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.131800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.111400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.005000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.077400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.930300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.983800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.961600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.963400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.075400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.060000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.126600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.041500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =       0.03\n",
      "  total_flos               =  1010115GF\n",
      "  train_loss               =     1.2045\n",
      "  train_runtime            = 0:03:21.41\n",
      "  train_samples_per_second =      0.397\n",
      "  train_steps_per_second   =      0.099\n",
      "{'train_runtime': 201.4107, 'train_samples_per_second': 0.397, 'train_steps_per_second': 0.099, 'total_flos': 1084603414462464.0, 'train_loss': 1.2045360773801803, 'epoch': 0.03}\n",
      "Saving last checkpoint of the model...\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"results/kisai_llama2/final_checkpoint\"\n",
    "train(model, tokenizer, dataset, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76bb78a-8dda-4ccd-8414-246552515b57",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Model save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "38438d91-a152-47d8-a174-98cb3e925c1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'results/kisai_llama2/final_checkpoint'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0767f413-56a5-4201-80ac-c0ac065a34fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoPeftModelForCausalLM.from_pretrained(output_dir, device_map=\"auto\", torch_dtype=torch.bfloat16)\n",
    "# model = model.merge_and_unload()\n",
    "\n",
    "output_merged_dir = \"results/llama2/final_merged_checkpoint\"\n",
    "os.makedirs(output_merged_dir, exist_ok=True)\n",
    "model.save_pretrained(output_merged_dir, safe_serialization=True)\n",
    "\n",
    "# save tokenizer for easy inference\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.save_pretrained(output_merged_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7e64dd-8d19-4250-95c5-86715b5c95d6",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb5822d-ee19-4440-825d-9bddf9b0f993",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset[\"input_ids\",])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5b785135-bfb1-4878-b992-3d5ff8ccf602",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7f9517d1da80>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "367ca9e4-d9de-45ff-baf9-84a2e793924b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_inference(instruction, context = None):\n",
    "    if context:\n",
    "        prompt = f\"Below is an instruction that describes a task, paired with an input that provides further context.\\n\\n### Instruction: \\n{instruction}\\n\\n### Input: \\n{context}\\n\\n### Response: \\n\"\n",
    "    else:\n",
    "        prompt = f\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction: \\n{instruction}\\n\\n### Response: \\n\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", return_token_type_ids=False).to(\"cuda:0\")\n",
    "    outputs = model.generate(**inputs, max_new_tokens=150)\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "    # New code to trim output\n",
    "    response = response.split('End of context:')[0]\n",
    "    \n",
    "    filtered_response = response.split(\"End of response:\")[0].split(\"Response:\")[1]\n",
    "    \n",
    "    # display(Markdown(response))\n",
    "    return filtered_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6bd14dd0-bb1c-4029-acc9-c2836a69170e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context.\n",
      "\n",
      "### Instruction: \n",
      "Summarize the following context. Your answer must be in English. End the response with the phrase 'End of response.'\n",
      "\n",
      "### Input: \n",
      "Today Sunday 3 September (20.00) Serbia-Turkey-Euro 2023 final of women’s volleyball. TO Brussels (Belgium) the continental title is up for grabs and it will be decided who will succeed Italy in the golden register. It promises to be a particularly balanced and compelling, exciting and intense challenge, open to any result. It will be a cross between Italian coaches, given that coach Daniele Santarelli sits on the Anatolian bench, capable of beating Italy in the tie-break, and Giovanni Guidetti is on the Balkan bench.\n",
      "\n",
      "### Response: \n",
      "Tonight the European women’s volleyball championship final between Serbia and Turkey. It will be decided who will succeed Italy in the golden register. It promises to be a particularly balanced and compelling, exciting and intense challenge, open to any result. It will be a cross between Italian coaches, given that coach Daniele Santarelli sits on the Anatolian bench, capable of beating Italy in the tie-break, and Giovanni Guidetti is on the Balkan bench.\n",
      "\n",
      "### End of response: \n",
      "End of response.\n",
      "\n",
      "### End of context:\n",
      "\n",
      "### End of input:\n",
      "\n",
      "### Response:\n",
      "\n",
      "### End\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Below is an instruction that describes a task, paired with an input that provides further context.\n",
       "\n",
       "### Instruction: \n",
       "Summarize the following context. Your answer must be in English. End the response with the phrase 'End of response.'\n",
       "\n",
       "### Input: \n",
       "Today Sunday 3 September (20.00) Serbia-Turkey-Euro 2023 final of women’s volleyball. TO Brussels (Belgium) the continental title is up for grabs and it will be decided who will succeed Italy in the golden register. It promises to be a particularly balanced and compelling, exciting and intense challenge, open to any result. It will be a cross between Italian coaches, given that coach Daniele Santarelli sits on the Anatolian bench, capable of beating Italy in the tie-break, and Giovanni Guidetti is on the Balkan bench.\n",
       "\n",
       "### Response: \n",
       "Tonight the European women’s volleyball championship final between Serbia and Turkey. It will be decided who will succeed Italy in the golden register. It promises to be a particularly balanced and compelling, exciting and intense challenge, open to any result. It will be a cross between Italian coaches, given that coach Daniele Santarelli sits on the Anatolian bench, capable of beating Italy in the tie-break, and Giovanni Guidetti is on the Balkan bench.\n",
       "\n",
       "### End of response: \n",
       "End of response.\n",
       "\n",
       "### "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Tonight the European women’s volleyball championship final between Serbia and Turkey. It will be decided who will succeed Italy in the golden register. It promises to be a particularly balanced and compelling, exciting and intense challenge, open to any result. It will be a cross between Italian coaches, given that coach Daniele Santarelli sits on the Anatolian bench, capable of beating Italy in the tie-break, and Giovanni Guidetti is on the Balkan bench.\n",
      "\n",
      "### \n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "answer = make_inference(instruction=\"Summarize the following context. Your answer must be in English. End the response with the phrase 'End of response.'\",  context=\"Today Sunday 3 September (20.00) Serbia-Turkey-Euro 2023 final of women’s volleyball. TO Brussels (Belgium) the continental title is up for grabs and it will be decided who will succeed Italy in the golden register. It promises to be a particularly balanced and compelling, exciting and intense challenge, open to any result. It will be a cross between Italian coaches, given that coach Daniele Santarelli sits on the Anatolian bench, capable of beating Italy in the tie-break, and Giovanni Guidetti is on the Balkan bench.\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c0780e2c-6560-4ec3-99df-ab74245ce915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n"
     ]
    }
   ],
   "source": [
    "print(\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cc62e433-0e13-47c4-936d-9b33611fe5ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Below is an instruction that describes a task, paired with an input that provides further context.\n",
       "\n",
       "### Instruction: \n",
       "Simplify the following context from CEFR level 3 to CEFR level 2. End the response with the phrase 'End of response.'\n",
       "\n",
       "### Input: \n",
       "Today Sunday 3 September (20.00) Serbia-Turkey-Euro 2023 final of women’s volleyball. TO Brussels (Belgium) the continental title is up for grabs and it will be decided who will succeed Italy in the golden register. It promises to be a particularly balanced and compelling, exciting and intense challenge, open to any result. It will be a cross between Italian coaches, given that coach Daniele Santarelli sits on the Anatolian bench, capable of beating Italy in the tie-break, and Giovanni Guidetti is on the Balkan bench.\n",
       "\n",
       "### Response: \n",
       "Today, the final of women’s volleyball is on. It is a match between Serbia and Turkey. Serbia beat Italy to get to the final. It will be a good match.\n",
       "\n",
       "### End of response: \n",
       "\n",
       "### "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "make_inference(instruction=\"Simplify the following context from CEFR level 3 to CEFR level 2. End the response with the phrase 'End of response.'\",  context=\"Today Sunday 3 September (20.00) Serbia-Turkey-Euro 2023 final of women’s volleyball. TO Brussels (Belgium) the continental title is up for grabs and it will be decided who will succeed Italy in the golden register. It promises to be a particularly balanced and compelling, exciting and intense challenge, open to any result. It will be a cross between Italian coaches, given that coach Daniele Santarelli sits on the Anatolian bench, capable of beating Italy in the tie-break, and Giovanni Guidetti is on the Balkan bench.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "45bef41b-9dc5-40a1-93a4-e50b8c9914f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Below is an instruction that describes a task, paired with an input that provides further context.\n",
       "\n",
       "### Instruction: \n",
       "Simplify the following context from CEFR level 2 to CEFR level 1. End the response with the phrase 'End of response.'\n",
       "\n",
       "### Input: \n",
       "Today Sunday 3 September (20.00) Serbia-Turkey-Euro 2023 final of women’s volleyball. TO Brussels (Belgium) the continental title is up for grabs and it will be decided who will succeed Italy in the golden register. It promises to be a particularly balanced and compelling, exciting and intense challenge, open to any result. It will be a cross between Italian coaches, given that coach Daniele Santarelli sits on the Anatolian bench, capable of beating Italy in the tie-break, and Giovanni Guidetti is on the Balkan bench.\n",
       "\n",
       "### Response: \n",
       "Today Sunday 3 September (20.00) Serbia-Turkey-Euro 2023 final of women’s volleyball. The continental title is up for grabs. It promises to be a particularly balanced and compelling challenge, open to any result. It will be a cross between Italian coaches, given that coach Daniele Santarelli sits on the Anatolian bench, capable of beating Italy in the tie-break, and Giovanni Guidetti is on the Balkan bench.\n",
       "\n",
       "### End of response: \n",
       "End of response.\n",
       "\n",
       "### "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "make_inference(instruction=\"Simplify the following context from CEFR level 2 to CEFR level 1. End the response with the phrase 'End of response.'\",  context=\"Today Sunday 3 September (20.00) Serbia-Turkey-Euro 2023 final of women’s volleyball. TO Brussels (Belgium) the continental title is up for grabs and it will be decided who will succeed Italy in the golden register. It promises to be a particularly balanced and compelling, exciting and intense challenge, open to any result. It will be a cross between Italian coaches, given that coach Daniele Santarelli sits on the Anatolian bench, capable of beating Italy in the tie-break, and Giovanni Guidetti is on the Balkan bench.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3dbb57ad-efc7-4d51-a377-e648904f0c94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Below is an instruction that describes a task, paired with an input that provides further context.\n",
       "\n",
       "### Instruction: \n",
       "When did Virgin Australia start operating?\n",
       "\n",
       "### Input: \n",
       "Virgin Australia, the trading name of Virgin Australia Airlines Pty Ltd, is an Australian-based airline. It is the largest airline by fleet size to use the Virgin brand. It commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route. It suddenly found itself as a major airline in Australia's domestic market after the collapse of Ansett Australia in September 2001. The airline has since grown to directly serve 32 cities in Australia, from hubs in Brisbane, Melbourne and Sydney.\n",
       "\n",
       "### Response: \n",
       "Virgin Australia started operating on August 31st, 2000.\n",
       "\n",
       "### "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "# ANSWER: Virgin Australia commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route.\n",
    "\n",
    "make_inference(instruction=\"When did Virgin Australia start operating?\",  \n",
    "               context=\"Virgin Australia, the trading name of Virgin Australia Airlines Pty Ltd, is an Australian-based airline. It is the largest airline by fleet size to use the Virgin brand. It commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route. It suddenly found itself as a major airline in Australia's domestic market after the collapse of Ansett Australia in September 2001. The airline has since grown to directly serve 32 cities in Australia, from hubs in Brisbane, Melbourne and Sydney.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "53b152bc-7b82-4f3f-9b9a-bd5322c80bc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Below is an instruction that describes a task, paired with an input that provides further context.\n",
       "\n",
       "### Instruction: \n",
       "If I have more pieces at the time of stalemate, have I won?\n",
       "\n",
       "### Input: \n",
       "Stalemate is a situation in chess where the player whose turn it is to move is not in check and has no legal move. Stalemate results in a draw. During the endgame, stalemate is a resource that can enable the player with the inferior position to draw the game rather than lose. In more complex positions, stalemate is much rarer, usually taking the form of a swindle that succeeds only if the superior side is inattentive.[citation needed] Stalemate is also a common theme in endgame studies and other chess problems. The outcome of a stalemate was standardized as a draw in the 19th century. Before this standardization, its treatment varied widely, including being deemed a win for the stalemating player, a half-win for that player, or a loss for that player; not being permitted; and resulting in the stalemated player missing a turn. Stalemate rules vary in other games of the chess family.\n",
       "\n",
       "### Response: \n",
       "No, I have not won.\n",
       "\n",
       "### Endpoint: \n",
       "Stalemate is a draw.\n",
       "\n",
       "### Context: \n",
       "Stalemate is a situation in chess where the player whose turn it is to move is not in check and has no legal move. Stalemate results in a draw. During the endgame, stalemate is a resource that can enable the player with the inferior position to draw the game rather than lose. In more complex positions, stalemate is much rarer, usually taking the form of a swindle that succeeds only if the superior side is inattentive. Stalemate is also a common theme in endgame studies and other chess problems. The outcome of a stalemate was standardized as a draw in the 19th century. Before this standardization, its treatment varied widely, including being deemed a win for the stalemating player, a half-win for that player, or a loss for that player; not being permitted; and resulting in the stalemated player missing a turn. Stalemate rules vary in other games of the chess family.\n",
       "\n",
       "### Endpoint: \n",
       "Stalem"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "# ANSWER: No. Stalemate is a drawn position. It doesn't matter who has captured more pieces or is in a winning position\n",
    "\n",
    "make_inference(instruction=\"If I have more pieces at the time of stalemate, have I won?\",  \n",
    "               context=\"Stalemate is a situation in chess where the player whose turn it is to move is not in check and has no legal move. Stalemate results in a draw. During the endgame, stalemate is a resource that can enable the player with the inferior position to draw the game rather than lose. In more complex positions, stalemate is much rarer, usually taking the form of a swindle that succeeds only if the superior side is inattentive.[citation needed] Stalemate is also a common theme in endgame studies and other chess problems. The outcome of a stalemate was standardized as a draw in the 19th century. Before this standardization, its treatment varied widely, including being deemed a win for the stalemating player, a half-win for that player, or a loss for that player; not being permitted; and resulting in the stalemated player missing a turn. Stalemate rules vary in other games of the chess family.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "01e46502-5c12-4e6e-8f3a-fa29d0ed56be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Below is an instruction that describes a task, paired with an input that provides further context.\n",
       "\n",
       "### Instruction: \n",
       "Given this paragraph, what is the top speed of a Kia Stinger?\n",
       "\n",
       "### Input: \n",
       "Kia claims that the Stinger accelerates from zero to 100 km/h (62 mph) in 7.7, 6 and 4.9 seconds for the 2.2-liter diesel, 2.0-liter petrol and 3.3-liter petrol respectively. Schreyer reportedly drove a pre-production Stinger GT at a top speed of 269 km/h (167 mph) on the Autobahn. During a test by Car and Driver, an all-wheel-drive U.S. spec GT 3.3T with Michelin Pilot Sport 4 tires achieved 0–60 mph (0–97 km/h) in 4.6 seconds on the track, reached 0.91 g on the skidpad and was able to stop from 70 mph (113 km/h) in 164 feet (50 m). According to this publication, the U.S. model's top speed is governed at 167 mph (269 km/h) per Kia specs. In tests conducted by Motor Trend, the four-cylinder U.S. spec Stinger 2.0 RWD on Bridgestone Potenza tires reached 60 mph (97 km/h) in 6.6 seconds, completed the 1⁄4-mile (0.4 km) run in 15 seconds and stopped from 60 mph (97 km/h) in 126 feet (38 m). The average lateral acceleration recorded in track testing was 0.85 g.\n",
       "\n",
       "### Response: \n",
       "The top speed of a Kia Stinger is 274 km/h (170 mph).\n",
       "\n",
       "### End:\n",
       "\n",
       "### End of Context:\n",
       "\n",
       "### End of Response:\n",
       "\n",
       "### Response: \n",
       "The top speed of a Kia Stinger is 274 km/h (170 mph).\n",
       "\n",
       "### End of Context:\n",
       "\n",
       "### End of Response:\n",
       "\n",
       "### Feedback:\n",
       "\n",
       "Good job!\n",
       "\n",
       "### End of Feedback:\n",
       "\n",
       "### Begin Extra Response:\n",
       "\n",
       "### Extra Response:\n",
       "\n",
       "### End of Extra Response:\n",
       "\n",
       "### End of Context:\n",
       "\n",
       "### End of Response:\n",
       "\n",
       "### Feedback:\n",
       "\n",
       "Very good job!\n",
       "\n",
       "### End of Feedback:\n",
       "\n",
       "### Begin Extra Response:\n",
       "\n",
       "### Extra Response:\n",
       "\n",
       "### End of Extra Response:\n",
       "\n",
       "### End of Context:\n",
       "\n",
       "### End of Response:\n",
       "\n",
       "### Feedback:\n",
       "\n",
       "Great job!\n",
       "\n",
       "### End of Feed"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "# ANSWER: The top speed of a Kia Stinger is 269km/h (167mph) according to this text.\n",
    "\n",
    "make_inference(instruction=\"Given this paragraph, what is the top speed of a Kia Stinger?\",  \n",
    "               context=\"Kia claims that the Stinger accelerates from zero to 100 km/h (62 mph) in 7.7, 6 and 4.9 seconds for the 2.2-liter diesel, 2.0-liter petrol and 3.3-liter petrol respectively. Schreyer reportedly drove a pre-production Stinger GT at a top speed of 269 km/h (167 mph) on the Autobahn. During a test by Car and Driver, an all-wheel-drive U.S. spec GT 3.3T with Michelin Pilot Sport 4 tires achieved 0–60 mph (0–97 km/h) in 4.6 seconds on the track, reached 0.91 g on the skidpad and was able to stop from 70 mph (113 km/h) in 164 feet (50 m). According to this publication, the U.S. model's top speed is governed at 167 mph (269 km/h) per Kia specs. In tests conducted by Motor Trend, the four-cylinder U.S. spec Stinger 2.0 RWD on Bridgestone Potenza tires reached 60 mph (97 km/h) in 6.6 seconds, completed the 1⁄4-mile (0.4 km) run in 15 seconds and stopped from 60 mph (97 km/h) in 126 feet (38 m). The average lateral acceleration recorded in track testing was 0.85 g.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4626f896-e2bb-4230-bef8-2619eb801731",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instruction</th>\n",
       "      <th>context</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Simplify the following context from CEFR Level...</td>\n",
       "      <td>Donald Trump is interested in buying Greenland...</td>\n",
       "      <td>Donald Trump is interested in buying Greenland...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Simplify the following context from CEFR Level...</td>\n",
       "      <td>Everyone knows that children don't like eating...</td>\n",
       "      <td>Everyone knows children don't like eating gree...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Simplify the following context from CEFR Level...</td>\n",
       "      <td>Archaeologists have found a big cemetery under...</td>\n",
       "      <td>Archaeologists found a big cemetery under the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Simplify the following context from CEFR Level...</td>\n",
       "      <td>Baby orangutans in Indonesia are going to scho...</td>\n",
       "      <td>Baby orangutans in Indonesia are going to scho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Simplify the following context from CEFR Level...</td>\n",
       "      <td>New research says bald men should not worry ab...</td>\n",
       "      <td>Research says bald men should not worry or get...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         instruction  \\\n",
       "0  Simplify the following context from CEFR Level...   \n",
       "1  Simplify the following context from CEFR Level...   \n",
       "2  Simplify the following context from CEFR Level...   \n",
       "3  Simplify the following context from CEFR Level...   \n",
       "4  Simplify the following context from CEFR Level...   \n",
       "\n",
       "                                             context  \\\n",
       "0  Donald Trump is interested in buying Greenland...   \n",
       "1  Everyone knows that children don't like eating...   \n",
       "2  Archaeologists have found a big cemetery under...   \n",
       "3  Baby orangutans in Indonesia are going to scho...   \n",
       "4  New research says bald men should not worry ab...   \n",
       "\n",
       "                                            response  \n",
       "0  Donald Trump is interested in buying Greenland...  \n",
       "1  Everyone knows children don't like eating gree...  \n",
       "2  Archaeologists found a big cemetery under the ...  \n",
       "3  Baby orangutans in Indonesia are going to scho...  \n",
       "4  Research says bald men should not worry or get...  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "53824b33-3371-4b17-a0a3-768e0b4ccfb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Donald Trump is interested in buying Greenland. He said it would be like buying property and was, \"a large real estate deal\". He added: \"They\\'ve got a lot of valuable minerals.\" Mr Trump said: \"Denmark owns it....We protect Denmark....So the concept came up and I said, \\'Certainly I\\'d be [interested].\\' He said buying Greenland was not his top priority at the moment. He said: \"It\\'s not number one on the burner.\"'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[0][\"context\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4948533a-8a72-45e6-961e-b5bd2452d54d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Donald Trump is interested in buying Greenland. He said it would be, \"a large real estate deal\". He added: \"They\\'ve got a lot of valuable minerals.\" Mr Trump said: \"Certainly I\\'d be [interested].\\' He said buying Greenland was not his top priority. He said: \"It\\'s not number one on the burner.\"'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[0][\"response\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ff27a1aa-f239-4c58-8871-cc495ba2504c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"model_response\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "543628a8-7054-42e9-a8db-3bbb96eb1898",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"gs://kisai-data-msca310019-capstone/Text_Simplification/finetuned_llama2_responses.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a16bf3-bb86-4fae-bf1a-e1e598a2f2e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently processing index  0\n",
      "Finished processing index 0 in 228.16458519599837 seconds.\n",
      "Saving for batch 0\n",
      "Finishes saving the batch 0 in 0.25415011800214415 seconds\n",
      "Currently processing index  1\n",
      "Finished processing index 1 in 216.31794119100232 seconds.\n",
      "Currently processing index  2\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "for idx, row in data.iterrows():\n",
    "    print(\"Currently processing index \", idx)\n",
    "    start = time.perf_counter()\n",
    "    answer = make_inference(instruction=row[\"instruction\"],  \n",
    "                            context=row[\"context\"])\n",
    "    data.at[idx, 'model_response'] = answer\n",
    "    end = time.perf_counter()\n",
    "\n",
    "    print(f\"Finished processing index {idx} in {end-start} seconds.\")\n",
    "    \n",
    "    if idx % 20 == 0:\n",
    "        print(f\"Saving for batch {idx//20}\")\n",
    "        start = time.perf_counter()\n",
    "        data.to_csv(PATH, index=False)\n",
    "        end = time.perf_counter()\n",
    "        print(f\"Finishes saving the batch {idx//20} in {end-start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5be2f7-0c03-4a82-a505-51fd9ab0729f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489cb04d-96ec-4562-abb9-4c0d4596abda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60530b17-e56d-4dd0-b98f-3d3438955185",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7529d4-9070-4b61-80ac-90c97a3b1b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b6d1e6-6394-4272-9ba6-9c9e5cc518ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.2-0.m111",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.2-0:m111"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
