{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa4483c6-f5ee-4812-aaaa-0b282c140174",
   "metadata": {},
   "source": [
    "### Fine-tuning LLama 2 with QLoRA\n",
    "https://blog.ovhcloud.com/fine-tuning-llama-2-models-using-a-single-gpu-qlora-and-ai-notebooks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "de9622d6-6105-499a-a3e5-921a1288b5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r requirements_ovhcloud.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e9fa3de9-9a50-44f8-8970-bc5941b3c58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !lsb_release -a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "71832104-9f40-4c40-90d6-8858d981d397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !export CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "69f2a5f9-ca7b-4871-bd5a-e29abbf5dc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !sudo chmod 777 /root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "858de0fb-4da7-4d01-8da7-4cc056394148",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92f29f14-c65c-4f96-9e6d-9a291cf1d543",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import bitsandbytes as bnb\n",
    "from datasets import load_dataset, Dataset\n",
    "from functools import partial\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, AutoPeftModelForCausalLM\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed, Trainer, TrainingArguments, BitsAndBytesConfig, \\\n",
    "    DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from peft import AutoPeftModelForCausalLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1231971-178d-40e1-a91c-e8520de2408c",
   "metadata": {},
   "source": [
    "### Download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "891f6f1b-1c64-4a1b-a133-31f397e461c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>source_level_og</th>\n",
       "      <th>target_level_og</th>\n",
       "      <th>data_source</th>\n",
       "      <th>data_type</th>\n",
       "      <th>source_level_cefr</th>\n",
       "      <th>target_level_cefr</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>British people are big tea drinkers. It is a t...</td>\n",
       "      <td>British people love tea. They drink it for dif...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>BreakingNewsEnglish</td>\n",
       "      <td>text_simplification</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TS000000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Many people around the world stay at home and ...</td>\n",
       "      <td>Many people stay at home. They do not want to ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>BreakingNewsEnglish</td>\n",
       "      <td>text_simplification</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TS000000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Most of us don't really take much notice of ca...</td>\n",
       "      <td>We rarely notice car license plates. Maybe we ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>BreakingNewsEnglish</td>\n",
       "      <td>text_simplification</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TS000000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Italy's ruling party may introduce a new law t...</td>\n",
       "      <td>Italy wants to stop people using English words...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>BreakingNewsEnglish</td>\n",
       "      <td>text_simplification</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TS000000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Some people are very forgetful, while others c...</td>\n",
       "      <td>Some people are forgetful, while others rememb...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>BreakingNewsEnglish</td>\n",
       "      <td>text_simplification</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TS000000005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              source  \\\n",
       "0  British people are big tea drinkers. It is a t...   \n",
       "1  Many people around the world stay at home and ...   \n",
       "2  Most of us don't really take much notice of ca...   \n",
       "3  Italy's ruling party may introduce a new law t...   \n",
       "4  Some people are very forgetful, while others c...   \n",
       "\n",
       "                                              target  source_level_og  \\\n",
       "0  British people love tea. They drink it for dif...              3.0   \n",
       "1  Many people stay at home. They do not want to ...              3.0   \n",
       "2  We rarely notice car license plates. Maybe we ...              3.0   \n",
       "3  Italy wants to stop people using English words...              3.0   \n",
       "4  Some people are forgetful, while others rememb...              3.0   \n",
       "\n",
       "   target_level_og          data_source            data_type  \\\n",
       "0              2.0  BreakingNewsEnglish  text_simplification   \n",
       "1              2.0  BreakingNewsEnglish  text_simplification   \n",
       "2              2.0  BreakingNewsEnglish  text_simplification   \n",
       "3              2.0  BreakingNewsEnglish  text_simplification   \n",
       "4              2.0  BreakingNewsEnglish  text_simplification   \n",
       "\n",
       "   source_level_cefr  target_level_cefr           id  \n",
       "0                NaN                NaN  TS000000001  \n",
       "1                NaN                NaN  TS000000002  \n",
       "2                NaN                NaN  TS000000003  \n",
       "3                NaN                NaN  TS000000004  \n",
       "4                NaN                NaN  TS000000005  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = \"gs://kisai-data-msca310019-capstone/Text_Simplification/simplified_df_cefr_labeled.csv\"\n",
    "PATH = \"gs://kisai-data-msca310019-capstone/raw_data.csv\"\n",
    "\n",
    "data = pd.read_csv(PATH)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9bff9f5f-506f-4f12-8c13-a59b7b4210d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "791159"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "783ad7fc-50f5-45fb-94ee-0dd81197dc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop([\"source_level_cefr\", \"target_level_cefr\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49df3e72-5179-4098-8f61-79d5a05507f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length before dropping nan 791159\n",
      "Length after dropping nan 12910\n"
     ]
    }
   ],
   "source": [
    "print(\"Length before dropping nan\", len(data))\n",
    "data.dropna(subset=['target_level_og', 'source_level_og'], inplace=True)\n",
    "print(\"Length after dropping nan\", len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d4a4d422-16d2-4395-a008-50f38a0fd732",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cefr_mapping\n",
       "2.0->1.0    4304\n",
       "3.0->2.0    4303\n",
       "3.0->1.0    4303\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"cefr_mapping\"] = data[\"source_level_og\"].astype(str) + \"->\" + data[\"target_level_og\"].astype(str)\n",
    "data[\"cefr_mapping\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "49cd5619-bdb9-4b5d-844f-bd18c116f19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_instruction_column(source_cefr, target_cefr):\n",
    "    return f\"Simplify the following context from CEFR Level {int(source_cefr)} to CEFR Level {int(target_cefr)}\"\n",
    "\n",
    "data[\"instruction\"] = data.apply(lambda row: generate_instruction_column(row[\"source_level_og\"], row[\"target_level_og\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f9fa3260-011d-4a1c-9c7a-6580d40171f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Simplify the following context from CEFR Level 3 to CEFR Level 2'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[0][\"instruction\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "20b90c44-7153-4b50-963b-248e7c149047",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.rename({'source': 'context', 'target': 'response'}, axis=1, inplace=True)\n",
    "data = data[[\"instruction\", \"context\", \"response\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "626c0885-e974-49d1-a39d-4b86773bd39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the DataFrame to a dictionary\n",
    "data_dict = data.to_dict('list')\n",
    "\n",
    "# Convert the dictionary to a Hugging Face Dataset\n",
    "dataset = Dataset.from_dict(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e86b2bf9-5609-45da-86aa-85dbfac8f7ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'context', 'response'],\n",
       "    num_rows: 12910\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c9ddc696-c466-4d01-af19-a325ddda598f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of prompts: 12910\n",
      "Column names are: ['instruction', 'context', 'response']\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of prompts: {len(dataset)}')\n",
    "print(f'Column names are: {dataset.column_names}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5be684f6-5c19-41c2-afa5-98338989946d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instruction</th>\n",
       "      <th>context</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Simplify the following context from CEFR Level...</td>\n",
       "      <td>British people are big tea drinkers. It is a t...</td>\n",
       "      <td>British people love tea. They drink it for dif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Simplify the following context from CEFR Level...</td>\n",
       "      <td>Many people around the world stay at home and ...</td>\n",
       "      <td>Many people stay at home. They do not want to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Simplify the following context from CEFR Level...</td>\n",
       "      <td>Most of us don't really take much notice of ca...</td>\n",
       "      <td>We rarely notice car license plates. Maybe we ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Simplify the following context from CEFR Level...</td>\n",
       "      <td>Italy's ruling party may introduce a new law t...</td>\n",
       "      <td>Italy wants to stop people using English words...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Simplify the following context from CEFR Level...</td>\n",
       "      <td>Some people are very forgetful, while others c...</td>\n",
       "      <td>Some people are forgetful, while others rememb...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         instruction  \\\n",
       "0  Simplify the following context from CEFR Level...   \n",
       "1  Simplify the following context from CEFR Level...   \n",
       "2  Simplify the following context from CEFR Level...   \n",
       "3  Simplify the following context from CEFR Level...   \n",
       "4  Simplify the following context from CEFR Level...   \n",
       "\n",
       "                                             context  \\\n",
       "0  British people are big tea drinkers. It is a t...   \n",
       "1  Many people around the world stay at home and ...   \n",
       "2  Most of us don't really take much notice of ca...   \n",
       "3  Italy's ruling party may introduce a new law t...   \n",
       "4  Some people are very forgetful, while others c...   \n",
       "\n",
       "                                            response  \n",
       "0  British people love tea. They drink it for dif...  \n",
       "1  Many people stay at home. They do not want to ...  \n",
       "2  We rarely notice car license plates. Maybe we ...  \n",
       "3  Italy wants to stop people using English words...  \n",
       "4  Some people are forgetful, while others rememb...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad53a0ba-18a3-4468-aa48-fc78529ff6dd",
   "metadata": {},
   "source": [
    "### Preprocess the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3a13a1d7-7514-4bae-bcaa-139d2bda1fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt_formats(sample):\n",
    "    \"\"\"\n",
    "    Format various fields of the sample ('instruction', 'context', 'response')\n",
    "    Then concatenate them using two newline characters \n",
    "    :param sample: Sample dictionnary\n",
    "    \"\"\"\n",
    "\n",
    "    INTRO_BLURB = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
    "    INSTRUCTION_KEY = \"### Instruction:\"\n",
    "    INPUT_KEY = \"Input:\"\n",
    "    RESPONSE_KEY = \"### Response:\"\n",
    "    END_KEY = \"### End\"\n",
    "    \n",
    "    blurb = f\"{INTRO_BLURB}\"\n",
    "    instruction = f\"{INSTRUCTION_KEY}\\n{sample['instruction']}\"\n",
    "    input_context = f\"{INPUT_KEY}\\n{sample['context']}\" if sample[\"context\"] else None\n",
    "    response = f\"{RESPONSE_KEY}\\n{sample['response']}\"\n",
    "    end = f\"{END_KEY}\"\n",
    "    \n",
    "    parts = [part for part in [blurb, instruction, input_context, response, end] if part]\n",
    "\n",
    "    formatted_prompt = \"\\n\\n\".join(parts)\n",
    "    \n",
    "    sample[\"text\"] = formatted_prompt\n",
    "\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2e082d55-bc40-486d-a88b-8fa7a3feab62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOURCE https://github.com/databrickslabs/dolly/blob/master/training/trainer.py\n",
    "def get_max_length(model):\n",
    "    conf = model.config\n",
    "    max_length = None\n",
    "    for length_setting in [\"n_positions\", \"max_position_embeddings\", \"seq_length\"]:\n",
    "        max_length = getattr(model.config, length_setting, None)\n",
    "        if max_length:\n",
    "            print(f\"Found max lenth: {max_length}\")\n",
    "            break\n",
    "    if not max_length:\n",
    "        max_length = 1024\n",
    "        print(f\"Using default max length: {max_length}\")\n",
    "    return max_length\n",
    "\n",
    "\n",
    "def preprocess_batch(batch, tokenizer, max_length):\n",
    "    \"\"\"\n",
    "    Tokenizing a batch\n",
    "    \"\"\"\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "\n",
    "# SOURCE https://github.com/databrickslabs/dolly/blob/master/training/trainer.py\n",
    "def preprocess_dataset(tokenizer: AutoTokenizer, max_length: int, seed, dataset: str):\n",
    "    \"\"\"Format & tokenize it so it is ready for training\n",
    "    :param tokenizer (AutoTokenizer): Model Tokenizer\n",
    "    :param max_length (int): Maximum number of tokens to emit from tokenizer\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add prompt to each sample\n",
    "    print(\"Preprocessing dataset...\")\n",
    "    dataset = dataset.map(create_prompt_formats)#, batched=True)\n",
    "    \n",
    "    # Apply preprocessing to each batch of the dataset & and remove 'instruction', 'context', 'response', 'category' fields\n",
    "    _preprocessing_function = partial(preprocess_batch, max_length=max_length, \n",
    "                                      tokenizer=tokenizer)\n",
    "    dataset = dataset.map(\n",
    "        _preprocessing_function,\n",
    "        batched=True,\n",
    "        remove_columns=[\"instruction\", \"context\", \"response\", \"text\"],\n",
    "    )\n",
    "\n",
    "    # Filter out samples that have input_ids exceeding max_length\n",
    "    dataset = dataset.filter(lambda sample: len(sample[\"input_ids\"]) < max_length)\n",
    "    \n",
    "    # Shuffle dataset\n",
    "    dataset = dataset.shuffle(seed=seed)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb15376-0114-46b7-9840-3bc16ce5e2f8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create a bitsandbytes configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e27bd92b-65ba-4b59-8428-cf83ca310a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bnb_config():\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    )\n",
    "\n",
    "    return bnb_config\n",
    "\n",
    "# SOURCE https://github.com/artidoro/qlora/blob/main/qlora.py\n",
    "\n",
    "def find_all_linear_names(model):\n",
    "    cls = bnb.nn.Linear4bit #if args.bits == 4 else (bnb.nn.Linear8bitLt if args.bits == 8 else torch.nn.Linear)\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "\n",
    "    if 'lm_head' in lora_module_names:  # needed for 16-bit\n",
    "        lora_module_names.remove('lm_head')\n",
    "    return list(lora_module_names)\n",
    "\n",
    "def create_peft_config(modules):\n",
    "    \"\"\"\n",
    "    Create Parameter-Efficient Fine-Tuning config for your model\n",
    "    :param modules: Names of the modules to apply Lora to\n",
    "    \"\"\"\n",
    "    config = LoraConfig(\n",
    "        r=8,  # dimension of the updated matrices\n",
    "        lora_alpha=16,  # parameter for scaling\n",
    "        target_modules=modules,\n",
    "        lora_dropout=0.1,  # dropout probability for layers\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "    print(\"Lora target_modules\", str(modules))\n",
    "\n",
    "    return config\n",
    "\n",
    "def print_trainable_parameters(model, use_4bit=False):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        num_params = param.numel()\n",
    "        # if using DS Zero 3 and the weights are initialized empty\n",
    "        if num_params == 0 and hasattr(param, \"ds_numel\"):\n",
    "            num_params = param.ds_numel\n",
    "\n",
    "        all_param += num_params\n",
    "        if param.requires_grad:\n",
    "            trainable_params += num_params\n",
    "    if use_4bit:\n",
    "        trainable_params /= 2\n",
    "    print(\n",
    "        f\"all params: {all_param:,d} || trainable params: {trainable_params:,d} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b3524ee7-e11a-4442-9276-77b8cb8c25c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m## Preprocess dataset\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m max_length \u001b[38;5;241m=\u001b[39m get_max_length(\u001b[43mmodel\u001b[49m)\n\u001b[1;32m      5\u001b[0m dataset \u001b[38;5;241m=\u001b[39m preprocess_dataset(tokenizer, max_length, seed, dataset)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "## Preprocess dataset\n",
    "\n",
    "max_length = get_max_length(model)\n",
    "\n",
    "dataset = preprocess_dataset(tokenizer, max_length, seed, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d519a5dd-091c-4db6-8ddb-eb5baaf5f9b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask'],\n",
       "    num_rows: 12910\n",
       "})"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6e4f2301-a0f2-4b6c-be72-20031add62c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"meta-llama/Llama-2-7b-hf\" \n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b07cd5-37b1-42b0-a20f-edf8bce43b94",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d810cd34-e96a-4ce7-883d-83cc92d34b96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5GB'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'{int(torch.cuda.mem_get_info()[0]/1024**3)-2}GB'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3b7f95fe-ec6e-4d5a-97df-3f57c93fcb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name, bnb_config):\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"cuda:0\", # dispatch efficiently the model on the available ressources  \n",
    "        load_in_8bit=True,\n",
    "        trust_remote_code=True,\n",
    "        #max_memory=f'{int(torch.cuda.mem_get_info()[0]/1024**3)-2}GB',\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def load_tokenizer(model_name):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=True)\n",
    "\n",
    "    # Needed for LLaMA tokenizer\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6cff6c67-3cd6-4082-8494-f2331f8751bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65680bdd02ba49b49420a40520064d41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:655: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load model from HF with user's token and with bitsandbytes config\n",
    "\n",
    "bnb_config = create_bnb_config()\n",
    "model = load_model(model_name, bnb_config)\n",
    "tokenizer = load_tokenizer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d70081d4-ab3c-4ec2-98bd-71335910b2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, tokenizer, dataset, output_dir):\n",
    "    model.gradient_checkpointing_enable()\n",
    "\n",
    "    # 2 - Using the prepare_model_for_kbit_training method from PEFT\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "    # Get lora module names\n",
    "    modules = find_all_linear_names(model)\n",
    "\n",
    "    # Create PEFT config for these modules and wrap the model to PEFT\n",
    "    peft_config = create_peft_config(modules)\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    \n",
    "    # Print information about the percentage of trainable parameters\n",
    "    print_trainable_parameters(model)\n",
    "    \n",
    "    # Training parameters\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        train_dataset=dataset,\n",
    "        args=TrainingArguments(\n",
    "            per_device_train_batch_size=4,\n",
    "            gradient_accumulation_steps=4,\n",
    "            num_train_epochs = 100,\n",
    "            max_steps=100,\n",
    "            learning_rate=2e-4,\n",
    "            fp16=True,\n",
    "            logging_steps=5,\n",
    "            output_dir=\"outputs\",\n",
    "            optim=\"paged_adamw_8bit\",\n",
    "        ),\n",
    "        data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "    )\n",
    "    \n",
    "    model.config.use_cache = False  # re-enable for inference to speed up predictions for similar inputs\n",
    "    \n",
    "    ### SOURCE https://github.com/artidoro/qlora/blob/main/qlora.py\n",
    "    # Verifying the datatypes before training\n",
    "    \n",
    "    dtypes = {}\n",
    "    for _, p in model.named_parameters():\n",
    "        dtype = p.dtype\n",
    "        if dtype not in dtypes: dtypes[dtype] = 0\n",
    "        dtypes[dtype] += p.numel()\n",
    "    total = 0\n",
    "    for k, v in dtypes.items(): total+= v\n",
    "    for k, v in dtypes.items():\n",
    "        print(k, v, v/total)\n",
    "     \n",
    "    do_train = True\n",
    "    \n",
    "    # Launch training\n",
    "    print(\"Training...\")\n",
    "    \n",
    "    if do_train:\n",
    "        train_result = trainer.train()\n",
    "        metrics = train_result.metrics\n",
    "        trainer.log_metrics(\"train\", metrics)\n",
    "        trainer.save_metrics(\"train\", metrics)\n",
    "        trainer.save_state()\n",
    "        print(metrics)    \n",
    "    \n",
    "    ###\n",
    "    \n",
    "    # Saving model\n",
    "    print(\"Saving last checkpoint of the model...\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    trainer.model.save_pretrained(output_dir)\n",
    "    \n",
    "    # Free memory for merging weights\n",
    "    del model\n",
    "    del trainer\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "fc69099a-80d7-429f-92ab-959889d5ecdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lora target_modules ['up_proj', 'v_proj', 'q_proj', 'gate_proj', 'o_proj', 'k_proj', 'down_proj']\n",
      "all params: 3,520,401,408 || trainable params: 19,988,480 || trainable%: 0.5677897967708119\n",
      "torch.float32 282398720 0.08021776134910578\n",
      "torch.uint8 3238002688 0.9197822386508943\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 50:32, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.797000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.476900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.380500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.309200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.294400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.240900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.246800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.283400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1.276400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.237800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>1.204600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.258400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>1.263800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.239100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>1.264900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.249600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>1.224900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.290800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>1.192300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.251200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =       0.12\n",
      "  total_flos               = 26981134GF\n",
      "  train_loss               =     1.2992\n",
      "  train_runtime            = 0:51:01.98\n",
      "  train_samples_per_second =      0.523\n",
      "  train_steps_per_second   =      0.033\n",
      "{'train_runtime': 3061.9885, 'train_samples_per_second': 0.523, 'train_steps_per_second': 0.033, 'total_flos': 2.897077254311117e+16, 'train_loss': 1.2991509819030762, 'epoch': 0.12}\n",
      "Saving last checkpoint of the model...\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"results/kisai_llama2/final_checkpoint\"\n",
    "train(model, tokenizer, dataset, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76bb78a-8dda-4ccd-8414-246552515b57",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Model save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "38438d91-a152-47d8-a174-98cb3e925c1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'results/kisai_llama2/final_checkpoint'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "daf24ec7-3e0c-4d28-8ae6-5f9f490d877a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jupyter/keep-it-simple-ai/text_simplification/llama-2-qlora'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfa9980-dd26-4d22-85b6-7bf44ba3a456",
   "metadata": {},
   "source": [
    "### Transfering the model to GCS\n",
    "\n",
    "gsutil cp -r keep-it-simple-ai/text_simplification/llama-2-qlora/results/kisai_llama2/final_checkpoint/ gs://kisai-data-msca310019-capstone/Text_Simplification/models/kisai-llama-2-4bit/version_2/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7e64dd-8d19-4250-95c5-86715b5c95d6",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "81ccb977-4cf3-45cb-87f4-214744f3a018",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"results/kisai_llama2/final_checkpoint\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1dc7e375-d33f-4eb3-a257-7cf0ecd5cac5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6c4591bab71431883f36ea37c0bc858",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    output_dir,\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map = \"auto\",\n",
    ")\n",
    "\n",
    "# Merge LoRA and base model\n",
    "# merged_model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bc85aa97-5993-4ff9-88bb-b6392b7855f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:655: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = load_tokenizer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5b785135-bfb1-4878-b992-3d5ff8ccf602",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7f9dddf03fd0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "1b935ca8-0cf3-4158-9c16-202d6e4771a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7bc96f0a-4565-481e-bf6c-fcd35ef0849b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "peft.peft_model.PeftModelForCausalLM"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "db41fad7-eb62-41f6-9a42-3444a9b02ac7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.llama.modeling_llama.LlamaForCausalLM"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(merged_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0d20da5a-04aa-482f-83b1-571d78ad1cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_model = torch.compile(merged_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "436a6d99-1a58-46f5-b019-aee0e7568cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaConfig {\n",
      "  \"_name_or_path\": \"meta-llama/Llama-2-7b-hf\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"quantization_config\": {\n",
      "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
      "    \"bnb_4bit_quant_type\": \"nf4\",\n",
      "    \"bnb_4bit_use_double_quant\": true,\n",
      "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "    \"llm_int8_has_fp16_weight\": false,\n",
      "    \"llm_int8_skip_modules\": null,\n",
      "    \"llm_int8_threshold\": 6.0,\n",
      "    \"load_in_4bit\": true,\n",
      "    \"load_in_8bit\": false,\n",
      "    \"quant_method\": \"bitsandbytes\"\n",
      "  },\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.35.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "merged_model.config.use_cache = True\n",
    "print(model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "367ca9e4-d9de-45ff-baf9-84a2e793924b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_inference(instruction, context = None):\n",
    "    if context:\n",
    "        prompt = f\"Below is an instruction that describes a task, paired with an input that provides further context.\\n\\n### Instruction: \\n{instruction}\\n\\n### Input: \\n{context}\\n\\n### Response: \\n\"\n",
    "    else:\n",
    "        prompt = f\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction: \\n{instruction}\\n\\n### Response: \\n\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", return_token_type_ids=False).to(\"cuda:0\")\n",
    "    outputs = model.generate(**inputs, max_new_tokens=150)\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "    # New code to trim output\n",
    "    response = response.split('End of context:')[0]\n",
    "    \n",
    "    filtered_response = response.split(\"End of response:\")[0].split(\"Response:\")[1]\n",
    "    \n",
    "    # display(Markdown(response))\n",
    "    return filtered_response\n",
    "\n",
    "\n",
    "def make_improvement_experimental(instruction, context=None):\n",
    "    if context: \n",
    "        prompt = f\"Below is an instruction that describes a task, paired with an input that provides further context.\\n\\n### Instruction: \\n{instruction}\\n\\n### Input: \\n{context}\\n\\n### Response: \\n\"\n",
    "    else:\n",
    "        prompt = f\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction: \\n{instruction}\\n\\n### Response: \\n\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", return_token_type_ids=False)\n",
    "    with torch.inference_mode():\n",
    "        outputs = model.generate(\n",
    "            input_ids = inputs.input_ids,\n",
    "            attention_mask = inputs.attention_mask,\n",
    "            do_sample = True,\n",
    "            max_new_tokens=250,\n",
    "            use_cache = True,\n",
    "        )\n",
    "    decoded_response = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "    return decoded_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "6bd14dd0-bb1c-4029-acc9-c2836a69170e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1min 37s ± 180 ms per loop (mean ± std. dev. of 3 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -r 3\n",
    "\n",
    "answer = make_improvement_experimental(instruction=\"Summarize the following context. Your answer must be in English. End the response with the phrase 'End of response.'\",  \n",
    "                        context=\"Today Sunday 3 September (20.00) Serbia-Turkey-Euro 2023 final of women’s volleyball. TO Brussels (Belgium) the continental title is up for grabs and it will be decided who will succeed Italy in the golden register. It promises to be a particularly balanced and compelling, exciting and intense challenge, open to any result. It will be a cross between Italian coaches, given that coach Daniele Santarelli sits on the Anatolian bench, capable of beating Italy in the tie-break, and Giovanni Guidetti is on the Balkan bench.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb33a14c-4580-4845-aefc-10063a698b07",
   "metadata": {},
   "source": [
    "### Compiling the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "bf3e35a8-0da2-44f2-8586-ff60ed788c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_improvement_experimental(instruction, context=None):\n",
    "    if context: \n",
    "        prompt = f\"Below is an instruction that describes a task, paired with an input that provides further context.\\n\\n### Instruction: \\n{instruction}\\n\\n### Input: \\n{context}\\n\\n### Response: \\n\"\n",
    "    else:\n",
    "        prompt = f\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction: \\n{instruction}\\n\\n### Response: \\n\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", return_token_type_ids=False)\n",
    "    with torch.inference_mode():\n",
    "        outputs = merged_model.generate(\n",
    "            input_ids = inputs.input_ids,\n",
    "            attention_mask = inputs.attention_mask,\n",
    "            do_sample = True,\n",
    "            max_new_tokens=250,\n",
    "            use_cache = True,\n",
    "        )\n",
    "    decoded_response = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "    return decoded_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700fe98c-7920-4c5d-b3d1-9b9b07a5a4a0",
   "metadata": {},
   "source": [
    "### Trying no_grad and .eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "f903176d-92db-4a6f-aa3d-b756e7bb615b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(instruction, context=None):\n",
    "    if context: \n",
    "        prompt = f\"Below is an instruction that describes a task, paired with an input that provides further context.\\n\\n### Instruction: \\n{instruction}\\n\\n### Input: \\n{context}\\n\\n### Response: \\n\"\n",
    "    else:\n",
    "        prompt = f\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction: \\n{instruction}\\n\\n### Response: \\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "902803ab-d335-45a0-9c75-f7f454d4843f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_improvement_experimental(instruction, context=None):\n",
    "    prompt = generate_prompt(instruction, context)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", return_token_type_ids=False)\n",
    "    merged_model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = merged_model.generate(\n",
    "            input_ids = inputs.input_ids,\n",
    "            attention_mask = inputs.attention_mask,\n",
    "            do_sample = True,\n",
    "            max_new_tokens=250,\n",
    "            use_cache = True,\n",
    "        )\n",
    "    decoded_response = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "    return decoded_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "ce2997fb-155c-48d2-a392-b217cb11ad60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1min 37s ± 446 ms per loop (mean ± std. dev. of 3 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -r 3\n",
    "\n",
    "answer = make_improvement_experimental(instruction=\"Summarize the following context. Your answer must be in English. End the response with the phrase 'End of response.'\",  \n",
    "                        context=\"Today Sunday 3 September (20.00) Serbia-Turkey-Euro 2023 final of women’s volleyball. TO Brussels (Belgium) the continental title is up for grabs and it will be decided who will succeed Italy in the golden register. It promises to be a particularly balanced and compelling, exciting and intense challenge, open to any result. It will be a cross between Italian coaches, given that coach Daniele Santarelli sits on the Anatolian bench, capable of beating Italy in the tie-break, and Giovanni Guidetti is on the Balkan bench.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f99503-8423-4aec-b3a8-da844f63f10c",
   "metadata": {},
   "source": [
    "### 2 GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "535b197a-8066-4842-b5fe-846e571bc572",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(instruction, context=None):\n",
    "    if context: \n",
    "        prompt = f\"Below is an instruction that describes a task, paired with an input that provides further context.\\n\\n### Instruction: \\n{instruction}\\n\\n### Input: \\n{context}\\n\\n### Response: \\n\"\n",
    "    else:\n",
    "        prompt = f\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction: \\n{instruction}\\n\\n### Response: \\n\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ee2409f8-797b-4a0c-b8d6-8b755841fe67",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction=\"Summarize the following context. Your answer must be in English. End the response with the phrase 'End of response.'\"  \n",
    "context=\"Today Sunday 3 September (20.00) Serbia-Turkey-Euro 2023 final of women’s volleyball. TO Brussels (Belgium) the continental title is up for grabs and it will be decided who will succeed Italy in the golden register. It promises to be a particularly balanced and compelling, exciting and intense challenge, open to any result. It will be a cross between Italian coaches, given that coach Daniele Santarelli sits on the Anatolian bench, capable of beating Italy in the tie-break, and Giovanni Guidetti is on the Balkan bench.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7ef151b4-9995-48b7-95b8-c3b8729469fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_improvement_experimental(instruction, context=None):\n",
    "    prompt = generate_prompt(instruction, context)\n",
    "    \n",
    "    inputs = tokenizer(text=prompt, return_tensors=\"pt\", return_token_type_ids=False).to(\"cuda\")\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids = inputs.input_ids,\n",
    "            attention_mask = inputs.attention_mask,\n",
    "            do_sample = True,\n",
    "            max_new_tokens=250,\n",
    "            use_cache = True,\n",
    "        )\n",
    "    decoded_response = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "    return decoded_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "37357435-4428-404a-91eb-4d5ad97b08fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.3 s ± 6.3 ms per loop (mean ± std. dev. of 3 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -r 3\n",
    "\n",
    "answer = make_improvement_experimental(instruction=instruction, context=context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0ccc92-7532-4850-8dea-3dc811a10909",
   "metadata": {},
   "source": [
    "### Inference speed comparison\n",
    "\n",
    "41.7 seconds with merged model \\\n",
    "29.3 seconds when the quantized model is saved, and reloaded as llama 2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d8dc21-641f-4019-a7e7-386f615b99d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e7aa3f-06f6-4cb6-9bba-7e320a23e419",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee043cb-f00f-4b4f-82bb-bfe2ed46b985",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54351b06-6deb-43da-9091-b05fc9228e0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10debf5c-2f80-4455-8e02-1124a40af68e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3ab97863-6db5-489b-adda-e1624a457862",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cc62e433-0e13-47c4-936d-9b33611fe5ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Below is an instruction that describes a task, paired with an input that provides further context.\n",
       "\n",
       "### Instruction: \n",
       "Simplify the following context from CEFR level 3 to CEFR level 2. End the response with the phrase 'End of response.'\n",
       "\n",
       "### Input: \n",
       "Today Sunday 3 September (20.00) Serbia-Turkey-Euro 2023 final of women’s volleyball. TO Brussels (Belgium) the continental title is up for grabs and it will be decided who will succeed Italy in the golden register. It promises to be a particularly balanced and compelling, exciting and intense challenge, open to any result. It will be a cross between Italian coaches, given that coach Daniele Santarelli sits on the Anatolian bench, capable of beating Italy in the tie-break, and Giovanni Guidetti is on the Balkan bench.\n",
       "\n",
       "### Response: \n",
       "Today, the final of women’s volleyball is on. It is a match between Serbia and Turkey. Serbia beat Italy to get to the final. It will be a good match.\n",
       "\n",
       "### End of response: \n",
       "\n",
       "### "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "make_inference(instruction=\"Simplify the following context from CEFR level 3 to CEFR level 2. End the response with the phrase 'End of response.'\",  context=\"Today Sunday 3 September (20.00) Serbia-Turkey-Euro 2023 final of women’s volleyball. TO Brussels (Belgium) the continental title is up for grabs and it will be decided who will succeed Italy in the golden register. It promises to be a particularly balanced and compelling, exciting and intense challenge, open to any result. It will be a cross between Italian coaches, given that coach Daniele Santarelli sits on the Anatolian bench, capable of beating Italy in the tie-break, and Giovanni Guidetti is on the Balkan bench.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "45bef41b-9dc5-40a1-93a4-e50b8c9914f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Below is an instruction that describes a task, paired with an input that provides further context.\n",
       "\n",
       "### Instruction: \n",
       "Simplify the following context from CEFR level 2 to CEFR level 1. End the response with the phrase 'End of response.'\n",
       "\n",
       "### Input: \n",
       "Today Sunday 3 September (20.00) Serbia-Turkey-Euro 2023 final of women’s volleyball. TO Brussels (Belgium) the continental title is up for grabs and it will be decided who will succeed Italy in the golden register. It promises to be a particularly balanced and compelling, exciting and intense challenge, open to any result. It will be a cross between Italian coaches, given that coach Daniele Santarelli sits on the Anatolian bench, capable of beating Italy in the tie-break, and Giovanni Guidetti is on the Balkan bench.\n",
       "\n",
       "### Response: \n",
       "Today Sunday 3 September (20.00) Serbia-Turkey-Euro 2023 final of women’s volleyball. The continental title is up for grabs. It promises to be a particularly balanced and compelling challenge, open to any result. It will be a cross between Italian coaches, given that coach Daniele Santarelli sits on the Anatolian bench, capable of beating Italy in the tie-break, and Giovanni Guidetti is on the Balkan bench.\n",
       "\n",
       "### End of response: \n",
       "End of response.\n",
       "\n",
       "### "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "make_inference(instruction=\"Simplify the following context from CEFR level 2 to CEFR level 1. End the response with the phrase 'End of response.'\",  context=\"Today Sunday 3 September (20.00) Serbia-Turkey-Euro 2023 final of women’s volleyball. TO Brussels (Belgium) the continental title is up for grabs and it will be decided who will succeed Italy in the golden register. It promises to be a particularly balanced and compelling, exciting and intense challenge, open to any result. It will be a cross between Italian coaches, given that coach Daniele Santarelli sits on the Anatolian bench, capable of beating Italy in the tie-break, and Giovanni Guidetti is on the Balkan bench.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3dbb57ad-efc7-4d51-a377-e648904f0c94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Below is an instruction that describes a task, paired with an input that provides further context.\n",
       "\n",
       "### Instruction: \n",
       "When did Virgin Australia start operating?\n",
       "\n",
       "### Input: \n",
       "Virgin Australia, the trading name of Virgin Australia Airlines Pty Ltd, is an Australian-based airline. It is the largest airline by fleet size to use the Virgin brand. It commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route. It suddenly found itself as a major airline in Australia's domestic market after the collapse of Ansett Australia in September 2001. The airline has since grown to directly serve 32 cities in Australia, from hubs in Brisbane, Melbourne and Sydney.\n",
       "\n",
       "### Response: \n",
       "Virgin Australia started operating on August 31st, 2000.\n",
       "\n",
       "### "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ANSWER: Virgin Australia commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route.\n",
    "\n",
    "make_inference(instruction=\"When did Virgin Australia start operating?\",  \n",
    "               context=\"Virgin Australia, the trading name of Virgin Australia Airlines Pty Ltd, is an Australian-based airline. It is the largest airline by fleet size to use the Virgin brand. It commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route. It suddenly found itself as a major airline in Australia's domestic market after the collapse of Ansett Australia in September 2001. The airline has since grown to directly serve 32 cities in Australia, from hubs in Brisbane, Melbourne and Sydney.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "53b152bc-7b82-4f3f-9b9a-bd5322c80bc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Below is an instruction that describes a task, paired with an input that provides further context.\n",
       "\n",
       "### Instruction: \n",
       "If I have more pieces at the time of stalemate, have I won?\n",
       "\n",
       "### Input: \n",
       "Stalemate is a situation in chess where the player whose turn it is to move is not in check and has no legal move. Stalemate results in a draw. During the endgame, stalemate is a resource that can enable the player with the inferior position to draw the game rather than lose. In more complex positions, stalemate is much rarer, usually taking the form of a swindle that succeeds only if the superior side is inattentive.[citation needed] Stalemate is also a common theme in endgame studies and other chess problems. The outcome of a stalemate was standardized as a draw in the 19th century. Before this standardization, its treatment varied widely, including being deemed a win for the stalemating player, a half-win for that player, or a loss for that player; not being permitted; and resulting in the stalemated player missing a turn. Stalemate rules vary in other games of the chess family.\n",
       "\n",
       "### Response: \n",
       "No, I have not won.\n",
       "\n",
       "### Endpoint: \n",
       "Stalemate is a draw.\n",
       "\n",
       "### Context: \n",
       "Stalemate is a situation in chess where the player whose turn it is to move is not in check and has no legal move. Stalemate results in a draw. During the endgame, stalemate is a resource that can enable the player with the inferior position to draw the game rather than lose. In more complex positions, stalemate is much rarer, usually taking the form of a swindle that succeeds only if the superior side is inattentive. Stalemate is also a common theme in endgame studies and other chess problems. The outcome of a stalemate was standardized as a draw in the 19th century. Before this standardization, its treatment varied widely, including being deemed a win for the stalemating player, a half-win for that player, or a loss for that player; not being permitted; and resulting in the stalemated player missing a turn. Stalemate rules vary in other games of the chess family.\n",
       "\n",
       "### Endpoint: \n",
       "Stalem"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ANSWER: No. Stalemate is a drawn position. It doesn't matter who has captured more pieces or is in a winning position\n",
    "\n",
    "make_inference(instruction=\"If I have more pieces at the time of stalemate, have I won?\",  \n",
    "               context=\"Stalemate is a situation in chess where the player whose turn it is to move is not in check and has no legal move. Stalemate results in a draw. During the endgame, stalemate is a resource that can enable the player with the inferior position to draw the game rather than lose. In more complex positions, stalemate is much rarer, usually taking the form of a swindle that succeeds only if the superior side is inattentive.[citation needed] Stalemate is also a common theme in endgame studies and other chess problems. The outcome of a stalemate was standardized as a draw in the 19th century. Before this standardization, its treatment varied widely, including being deemed a win for the stalemating player, a half-win for that player, or a loss for that player; not being permitted; and resulting in the stalemated player missing a turn. Stalemate rules vary in other games of the chess family.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "01e46502-5c12-4e6e-8f3a-fa29d0ed56be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Below is an instruction that describes a task, paired with an input that provides further context.\n",
       "\n",
       "### Instruction: \n",
       "Given this paragraph, what is the top speed of a Kia Stinger?\n",
       "\n",
       "### Input: \n",
       "Kia claims that the Stinger accelerates from zero to 100 km/h (62 mph) in 7.7, 6 and 4.9 seconds for the 2.2-liter diesel, 2.0-liter petrol and 3.3-liter petrol respectively. Schreyer reportedly drove a pre-production Stinger GT at a top speed of 269 km/h (167 mph) on the Autobahn. During a test by Car and Driver, an all-wheel-drive U.S. spec GT 3.3T with Michelin Pilot Sport 4 tires achieved 0–60 mph (0–97 km/h) in 4.6 seconds on the track, reached 0.91 g on the skidpad and was able to stop from 70 mph (113 km/h) in 164 feet (50 m). According to this publication, the U.S. model's top speed is governed at 167 mph (269 km/h) per Kia specs. In tests conducted by Motor Trend, the four-cylinder U.S. spec Stinger 2.0 RWD on Bridgestone Potenza tires reached 60 mph (97 km/h) in 6.6 seconds, completed the 1⁄4-mile (0.4 km) run in 15 seconds and stopped from 60 mph (97 km/h) in 126 feet (38 m). The average lateral acceleration recorded in track testing was 0.85 g.\n",
       "\n",
       "### Response: \n",
       "The top speed of a Kia Stinger is 274 km/h (170 mph).\n",
       "\n",
       "### End:\n",
       "\n",
       "### End of Context:\n",
       "\n",
       "### End of Response:\n",
       "\n",
       "### Response: \n",
       "The top speed of a Kia Stinger is 274 km/h (170 mph).\n",
       "\n",
       "### End of Context:\n",
       "\n",
       "### End of Response:\n",
       "\n",
       "### Feedback:\n",
       "\n",
       "Good job!\n",
       "\n",
       "### End of Feedback:\n",
       "\n",
       "### Begin Extra Response:\n",
       "\n",
       "### Extra Response:\n",
       "\n",
       "### End of Extra Response:\n",
       "\n",
       "### End of Context:\n",
       "\n",
       "### End of Response:\n",
       "\n",
       "### Feedback:\n",
       "\n",
       "Very good job!\n",
       "\n",
       "### End of Feedback:\n",
       "\n",
       "### Begin Extra Response:\n",
       "\n",
       "### Extra Response:\n",
       "\n",
       "### End of Extra Response:\n",
       "\n",
       "### End of Context:\n",
       "\n",
       "### End of Response:\n",
       "\n",
       "### Feedback:\n",
       "\n",
       "Great job!\n",
       "\n",
       "### End of Feed"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ANSWER: The top speed of a Kia Stinger is 269km/h (167mph) according to this text.\n",
    "\n",
    "make_inference(instruction=\"Given this paragraph, what is the top speed of a Kia Stinger?\",  \n",
    "               context=\"Kia claims that the Stinger accelerates from zero to 100 km/h (62 mph) in 7.7, 6 and 4.9 seconds for the 2.2-liter diesel, 2.0-liter petrol and 3.3-liter petrol respectively. Schreyer reportedly drove a pre-production Stinger GT at a top speed of 269 km/h (167 mph) on the Autobahn. During a test by Car and Driver, an all-wheel-drive U.S. spec GT 3.3T with Michelin Pilot Sport 4 tires achieved 0–60 mph (0–97 km/h) in 4.6 seconds on the track, reached 0.91 g on the skidpad and was able to stop from 70 mph (113 km/h) in 164 feet (50 m). According to this publication, the U.S. model's top speed is governed at 167 mph (269 km/h) per Kia specs. In tests conducted by Motor Trend, the four-cylinder U.S. spec Stinger 2.0 RWD on Bridgestone Potenza tires reached 60 mph (97 km/h) in 6.6 seconds, completed the 1⁄4-mile (0.4 km) run in 15 seconds and stopped from 60 mph (97 km/h) in 126 feet (38 m). The average lateral acceleration recorded in track testing was 0.85 g.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162bca92-c849-49f4-80b9-35ef598dfb18",
   "metadata": {},
   "source": [
    "### Inferencing on Text Simplification Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4626f896-e2bb-4230-bef8-2619eb801731",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instruction</th>\n",
       "      <th>context</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Simplify the following context from CEFR Level...</td>\n",
       "      <td>British people are big tea drinkers. It is a t...</td>\n",
       "      <td>British people love tea. They drink it for dif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Simplify the following context from CEFR Level...</td>\n",
       "      <td>Many people around the world stay at home and ...</td>\n",
       "      <td>Many people stay at home. They do not want to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Simplify the following context from CEFR Level...</td>\n",
       "      <td>Most of us don't really take much notice of ca...</td>\n",
       "      <td>We rarely notice car license plates. Maybe we ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Simplify the following context from CEFR Level...</td>\n",
       "      <td>Italy's ruling party may introduce a new law t...</td>\n",
       "      <td>Italy wants to stop people using English words...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Simplify the following context from CEFR Level...</td>\n",
       "      <td>Some people are very forgetful, while others c...</td>\n",
       "      <td>Some people are forgetful, while others rememb...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         instruction  \\\n",
       "0  Simplify the following context from CEFR Level...   \n",
       "1  Simplify the following context from CEFR Level...   \n",
       "2  Simplify the following context from CEFR Level...   \n",
       "3  Simplify the following context from CEFR Level...   \n",
       "4  Simplify the following context from CEFR Level...   \n",
       "\n",
       "                                             context  \\\n",
       "0  British people are big tea drinkers. It is a t...   \n",
       "1  Many people around the world stay at home and ...   \n",
       "2  Most of us don't really take much notice of ca...   \n",
       "3  Italy's ruling party may introduce a new law t...   \n",
       "4  Some people are very forgetful, while others c...   \n",
       "\n",
       "                                            response  \n",
       "0  British people love tea. They drink it for dif...  \n",
       "1  Many people stay at home. They do not want to ...  \n",
       "2  We rarely notice car license plates. Maybe we ...  \n",
       "3  Italy wants to stop people using English words...  \n",
       "4  Some people are forgetful, while others rememb...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "53824b33-3371-4b17-a0a3-768e0b4ccfb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'British people are big tea drinkers. It is a tradition in Britain to drink tea for different occasions and reasons. People have it for breakfast, for when guests visit, and for tea breaks at work. People even \"have a cuppa\" when they talk about their personal problems. However, research from The Tea Group shows that herbal, fruit and other teas have become more popular than traditional English breakfast tea. Researchers conducted a survey of more than 2,000 tea lovers. Over half of people said their favourite tea was not the traditional variety. Over a fifth of people chose green tea as their favourite brew. Just over 20 per cent said Earl Grey was their number one.\\r\\n\\r\\nSales of traditional tea in the U.K. have been declining. Three years ago, a survey found that 54 per cent of Britons preferred English breakfast tea. The new research shows that breakfast tea is likely to continue to decline in popularity. The researchers found many other things about tea-drinking habits in the U.K. The biggest reason for drinking tea was to relax. A quarter of Britons drink up to 10 cups a day. Brits seem to love milky and sugary tea. Around 85 per cent of people who drink Earl Grey and English breakfast put milk in it. Nearly 45 per cent of people sweeten their tea with sugar. Amazingly, people with a sweet tooth put three teaspoons of sugar in their cup.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[0][\"context\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4948533a-8a72-45e6-961e-b5bd2452d54d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'British people love tea. They drink it for different reasons – for breakfast, to give to guests, for tea breaks at work, and even when talking about their problems. New research shows that herbal, fruit and other teas are more popular than English breakfast tea. A survey of over 2,000 tea lovers showed less than half of people said they preferred traditional tea. Over a fifth of people said green tea was their favourite.\\r\\n\\r\\nSales of traditional tea have fallen. The research shows this will continue. The researchers found many other things about tea-drinking in the UK. The biggest reason for drinking tea was to relax. A quarter of people drink up to 10 cups a day. British people love milky and sugary tea. Nearly 45 per cent of them sweeten their tea with sugar. People with a sweet tooth put three teaspoons of sugar in their tea.'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[0][\"response\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bc787dc2-3bca-4c86-9e6a-eee9ace7e4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_inference(instruction, context=None):\n",
    "    prompt = generate_prompt(instruction, context)\n",
    "    \n",
    "    inputs = tokenizer(text=prompt, return_tensors=\"pt\", return_token_type_ids=False).to(\"cuda\")\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids = inputs.input_ids,\n",
    "            attention_mask = inputs.attention_mask,\n",
    "            do_sample = True,\n",
    "            max_new_tokens=250,\n",
    "            use_cache = True,\n",
    "        )\n",
    "    decoded_response = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "    return decoded_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ff27a1aa-f239-4c58-8871-cc495ba2504c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"model_response\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "543628a8-7054-42e9-a8db-3bbb96eb1898",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"gs://kisai-data-msca310019-capstone/Text_Simplification/finetuned_llama2_responses.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "44beeef0-fc06-42df-9c04-40d837dbddd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_v2 = pd.read_csv(PATH)\n",
    "data_v2[\"model_v2_response\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fce85c3a-ad72-401a-84f3-7db71860db3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instruction</th>\n",
       "      <th>context</th>\n",
       "      <th>response</th>\n",
       "      <th>model_response</th>\n",
       "      <th>oob_model_response</th>\n",
       "      <th>model_v2_response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Simplify the following context from CEFR Level...</td>\n",
       "      <td>Donald Trump is interested in buying Greenland...</td>\n",
       "      <td>Donald Trump is interested in buying Greenland...</td>\n",
       "      <td>\\nDonald Trump is interested in buying Greenl...</td>\n",
       "      <td>\\nDonald Trump is interested in buying Greenl...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Simplify the following context from CEFR Level...</td>\n",
       "      <td>Everyone knows that children don't like eating...</td>\n",
       "      <td>Everyone knows children don't like eating gree...</td>\n",
       "      <td>\\nChildren don't like eating greens. Parents ...</td>\n",
       "      <td>\\nEveryone knows that children don't like eat...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Simplify the following context from CEFR Level...</td>\n",
       "      <td>Archaeologists have found a big cemetery under...</td>\n",
       "      <td>Archaeologists found a big cemetery under the ...</td>\n",
       "      <td>\\nArchaeologists have found a big cemetery un...</td>\n",
       "      <td>\\nThe text has been simplified from CEFR Leve...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Simplify the following context from CEFR Level...</td>\n",
       "      <td>Baby orangutans in Indonesia are going to scho...</td>\n",
       "      <td>Baby orangutans in Indonesia are going to scho...</td>\n",
       "      <td>\\nBaby orangutans in Indonesia are going to s...</td>\n",
       "      <td>\\nAfter seven or eight years, the young apes ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Simplify the following context from CEFR Level...</td>\n",
       "      <td>New research says bald men should not worry ab...</td>\n",
       "      <td>Research says bald men should not worry or get...</td>\n",
       "      <td>\\nBald men should not worry about their looks...</td>\n",
       "      <td>\\nThe bald men were rated as more attractive,...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         instruction  \\\n",
       "0  Simplify the following context from CEFR Level...   \n",
       "1  Simplify the following context from CEFR Level...   \n",
       "2  Simplify the following context from CEFR Level...   \n",
       "3  Simplify the following context from CEFR Level...   \n",
       "4  Simplify the following context from CEFR Level...   \n",
       "\n",
       "                                             context  \\\n",
       "0  Donald Trump is interested in buying Greenland...   \n",
       "1  Everyone knows that children don't like eating...   \n",
       "2  Archaeologists have found a big cemetery under...   \n",
       "3  Baby orangutans in Indonesia are going to scho...   \n",
       "4  New research says bald men should not worry ab...   \n",
       "\n",
       "                                            response  \\\n",
       "0  Donald Trump is interested in buying Greenland...   \n",
       "1  Everyone knows children don't like eating gree...   \n",
       "2  Archaeologists found a big cemetery under the ...   \n",
       "3  Baby orangutans in Indonesia are going to scho...   \n",
       "4  Research says bald men should not worry or get...   \n",
       "\n",
       "                                      model_response  \\\n",
       "0   \\nDonald Trump is interested in buying Greenl...   \n",
       "1   \\nChildren don't like eating greens. Parents ...   \n",
       "2   \\nArchaeologists have found a big cemetery un...   \n",
       "3   \\nBaby orangutans in Indonesia are going to s...   \n",
       "4   \\nBald men should not worry about their looks...   \n",
       "\n",
       "                                  oob_model_response model_v2_response  \n",
       "0   \\nDonald Trump is interested in buying Greenl...                    \n",
       "1   \\nEveryone knows that children don't like eat...                    \n",
       "2   \\nThe text has been simplified from CEFR Leve...                    \n",
       "3   \\nAfter seven or eight years, the young apes ...                    \n",
       "4   \\nThe bald men were rated as more attractive,...                    "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_v2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a16bf3-bb86-4fae-bf1a-e1e598a2f2e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently processing index  0\n",
      "Finished processing index 0 in 44.44794289399999 seconds.\n",
      "Saving for batch 0\n",
      "Finishes saving the batch 0 in 0.3567352040000742 seconds\n",
      "Currently processing index  1\n",
      "Finished processing index 1 in 31.098942822000026 seconds.\n",
      "Currently processing index  2\n",
      "Finished processing index 2 in 31.23130393100007 seconds.\n",
      "Currently processing index  3\n",
      "Finished processing index 3 in 31.160384366000017 seconds.\n",
      "Currently processing index  4\n",
      "Finished processing index 4 in 10.434447689999956 seconds.\n",
      "Currently processing index  5\n",
      "Finished processing index 5 in 14.49660225699995 seconds.\n",
      "Currently processing index  6\n",
      "Finished processing index 6 in 31.131434032000016 seconds.\n",
      "Currently processing index  7\n",
      "Finished processing index 7 in 19.292316589999928 seconds.\n",
      "Currently processing index  8\n",
      "Finished processing index 8 in 31.24450003000004 seconds.\n",
      "Currently processing index  9\n",
      "Finished processing index 9 in 32.066916752 seconds.\n",
      "Currently processing index  10\n",
      "Finished processing index 10 in 31.21538327499991 seconds.\n",
      "Currently processing index  11\n",
      "Finished processing index 11 in 31.19378209499996 seconds.\n",
      "Currently processing index  12\n",
      "Finished processing index 12 in 31.163081044000137 seconds.\n",
      "Currently processing index  13\n",
      "Finished processing index 13 in 31.2270889859999 seconds.\n",
      "Currently processing index  14\n",
      "Finished processing index 14 in 31.153503248000106 seconds.\n",
      "Currently processing index  15\n",
      "Finished processing index 15 in 31.211904466000078 seconds.\n",
      "Currently processing index  16\n",
      "Finished processing index 16 in 18.079508655000154 seconds.\n",
      "Currently processing index  17\n",
      "Finished processing index 17 in 31.10031790500011 seconds.\n",
      "Currently processing index  18\n",
      "Finished processing index 18 in 31.238566153999955 seconds.\n",
      "Currently processing index  19\n",
      "Finished processing index 19 in 31.133229905999997 seconds.\n",
      "Currently processing index  20\n",
      "Finished processing index 20 in 31.04760931199985 seconds.\n",
      "Saving for batch 1\n",
      "Finishes saving the batch 1 in 0.3494377050001276 seconds\n",
      "Currently processing index  21\n",
      "Finished processing index 21 in 31.953385802999946 seconds.\n",
      "Currently processing index  22\n",
      "Finished processing index 22 in 31.952696069000012 seconds.\n",
      "Currently processing index  23\n",
      "Finished processing index 23 in 31.10660960799987 seconds.\n",
      "Currently processing index  24\n",
      "Finished processing index 24 in 31.483260630999894 seconds.\n",
      "Currently processing index  25\n",
      "Finished processing index 25 in 31.376738363999948 seconds.\n",
      "Currently processing index  26\n",
      "Finished processing index 26 in 31.222452578000002 seconds.\n",
      "Currently processing index  27\n",
      "Finished processing index 27 in 31.423497177999934 seconds.\n",
      "Currently processing index  28\n",
      "Finished processing index 28 in 31.44034571499992 seconds.\n",
      "Currently processing index  29\n",
      "Finished processing index 29 in 31.165859533000003 seconds.\n",
      "Currently processing index  30\n",
      "Finished processing index 30 in 31.45440003800013 seconds.\n",
      "Currently processing index  31\n",
      "Finished processing index 31 in 31.20673392399999 seconds.\n",
      "Currently processing index  32\n",
      "Finished processing index 32 in 32.03762391000009 seconds.\n",
      "Currently processing index  33\n",
      "Finished processing index 33 in 31.151961151000023 seconds.\n",
      "Currently processing index  34\n",
      "Finished processing index 34 in 31.463046560000066 seconds.\n",
      "Currently processing index  35\n",
      "Finished processing index 35 in 31.15520182499995 seconds.\n",
      "Currently processing index  36\n",
      "Finished processing index 36 in 31.107350731999986 seconds.\n",
      "Currently processing index  37\n",
      "Finished processing index 37 in 31.10873940500005 seconds.\n",
      "Currently processing index  38\n",
      "Finished processing index 38 in 31.12035201599997 seconds.\n",
      "Currently processing index  39\n",
      "Finished processing index 39 in 31.08292787000005 seconds.\n",
      "Currently processing index  40\n",
      "Finished processing index 40 in 31.979017378000208 seconds.\n",
      "Saving for batch 2\n",
      "Finishes saving the batch 2 in 0.3564975469998899 seconds\n",
      "Currently processing index  41\n",
      "Finished processing index 41 in 31.093814733000045 seconds.\n",
      "Currently processing index  42\n",
      "Finished processing index 42 in 32.0153168920001 seconds.\n",
      "Currently processing index  43\n",
      "Finished processing index 43 in 31.215845053000066 seconds.\n",
      "Currently processing index  44\n",
      "Finished processing index 44 in 31.109562538999853 seconds.\n",
      "Currently processing index  45\n",
      "Finished processing index 45 in 31.886440439000125 seconds.\n",
      "Currently processing index  46\n",
      "Finished processing index 46 in 31.168820563999816 seconds.\n",
      "Currently processing index  47\n",
      "Finished processing index 47 in 9.860683951000283 seconds.\n",
      "Currently processing index  48\n",
      "Finished processing index 48 in 31.058760517999872 seconds.\n",
      "Currently processing index  49\n",
      "Finished processing index 49 in 18.847968090999984 seconds.\n",
      "Currently processing index  50\n",
      "Finished processing index 50 in 31.127166859000226 seconds.\n",
      "Currently processing index  51\n",
      "Finished processing index 51 in 10.970127672999752 seconds.\n",
      "Currently processing index  52\n",
      "Finished processing index 52 in 31.17388917800008 seconds.\n",
      "Currently processing index  53\n",
      "Finished processing index 53 in 31.159529099999872 seconds.\n",
      "Currently processing index  54\n",
      "Finished processing index 54 in 32.03876853899965 seconds.\n",
      "Currently processing index  55\n",
      "Finished processing index 55 in 31.194680307 seconds.\n",
      "Currently processing index  56\n",
      "Finished processing index 56 in 31.144880561000264 seconds.\n",
      "Currently processing index  57\n",
      "Finished processing index 57 in 31.971762841999862 seconds.\n",
      "Currently processing index  58\n",
      "Finished processing index 58 in 31.116797052000038 seconds.\n",
      "Currently processing index  59\n",
      "Finished processing index 59 in 31.09190623600034 seconds.\n",
      "Currently processing index  60\n",
      "Finished processing index 60 in 31.08424928600016 seconds.\n",
      "Saving for batch 3\n",
      "Finishes saving the batch 3 in 0.3426007479997679 seconds\n",
      "Currently processing index  61\n",
      "Finished processing index 61 in 31.137418979999893 seconds.\n",
      "Currently processing index  62\n",
      "Finished processing index 62 in 31.170793819999744 seconds.\n",
      "Currently processing index  63\n",
      "Finished processing index 63 in 31.15528466900014 seconds.\n",
      "Currently processing index  64\n",
      "Finished processing index 64 in 31.06952336700033 seconds.\n",
      "Currently processing index  65\n",
      "Finished processing index 65 in 31.07642376900003 seconds.\n",
      "Currently processing index  66\n",
      "Finished processing index 66 in 31.10233880900023 seconds.\n",
      "Currently processing index  67\n",
      "Finished processing index 67 in 31.15807802900008 seconds.\n",
      "Currently processing index  68\n",
      "Finished processing index 68 in 17.906211362999784 seconds.\n",
      "Currently processing index  69\n",
      "Finished processing index 69 in 31.177295678000064 seconds.\n",
      "Currently processing index  70\n",
      "Finished processing index 70 in 31.089715094999974 seconds.\n",
      "Currently processing index  71\n",
      "Finished processing index 71 in 31.988823280999895 seconds.\n",
      "Currently processing index  72\n",
      "Finished processing index 72 in 31.16808226100011 seconds.\n",
      "Currently processing index  73\n",
      "Finished processing index 73 in 31.13554185399971 seconds.\n",
      "Currently processing index  74\n",
      "Finished processing index 74 in 31.209962041999916 seconds.\n",
      "Currently processing index  75\n",
      "Finished processing index 75 in 31.082323610000003 seconds.\n",
      "Currently processing index  76\n",
      "Finished processing index 76 in 14.201332360999913 seconds.\n",
      "Currently processing index  77\n",
      "Finished processing index 77 in 32.072899849999885 seconds.\n",
      "Currently processing index  78\n",
      "Finished processing index 78 in 31.105105363999883 seconds.\n",
      "Currently processing index  79\n",
      "Finished processing index 79 in 8.470113748999665 seconds.\n",
      "Currently processing index  80\n",
      "Finished processing index 80 in 9.213124487000186 seconds.\n",
      "Saving for batch 4\n",
      "Finishes saving the batch 4 in 0.35787964900009683 seconds\n",
      "Currently processing index  81\n",
      "Finished processing index 81 in 31.510627131000092 seconds.\n",
      "Currently processing index  82\n",
      "Finished processing index 82 in 31.5576714419999 seconds.\n",
      "Currently processing index  83\n",
      "Finished processing index 83 in 33.10571368899991 seconds.\n",
      "Currently processing index  84\n",
      "Finished processing index 84 in 31.575898059999872 seconds.\n",
      "Currently processing index  85\n",
      "Finished processing index 85 in 17.487747477000084 seconds.\n",
      "Currently processing index  86\n",
      "Finished processing index 86 in 31.50249125399978 seconds.\n",
      "Currently processing index  87\n",
      "Finished processing index 87 in 31.567438404999848 seconds.\n",
      "Currently processing index  88\n",
      "Finished processing index 88 in 31.59176732800006 seconds.\n",
      "Currently processing index  89\n",
      "Finished processing index 89 in 31.541629188000115 seconds.\n",
      "Currently processing index  90\n",
      "Finished processing index 90 in 31.575173183999595 seconds.\n",
      "Currently processing index  91\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "for idx, row in data_v2.iterrows():\n",
    "    print(\"Currently processing index \", idx)\n",
    "    start = time.perf_counter()\n",
    "    answer = make_inference(instruction=row[\"instruction\"],  \n",
    "                            context=row[\"context\"])\n",
    "    data_v2.at[idx, 'model_v2_response'] = answer\n",
    "    end = time.perf_counter()\n",
    "\n",
    "    print(f\"Finished processing index {idx} in {end-start} seconds.\")\n",
    "    \n",
    "    if idx % 20 == 0:\n",
    "        print(f\"Saving for batch {idx//20}\")\n",
    "        start = time.perf_counter()\n",
    "        data_v2.to_csv(PATH, index=False)\n",
    "        end = time.perf_counter()\n",
    "        print(f\"Finishes saving the batch {idx//20} in {end-start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5be2f7-0c03-4a82-a505-51fd9ab0729f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489cb04d-96ec-4562-abb9-4c0d4596abda",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60530b17-e56d-4dd0-b98f-3d3438955185",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7529d4-9070-4b61-80ac-90c97a3b1b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b6d1e6-6394-4272-9ba6-9c9e5cc518ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.2-0.m111",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.2-0:m111"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
